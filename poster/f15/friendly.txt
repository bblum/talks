title: Systematic Testing with Data-Race Preemption Points
authors: Ben Blum and Garth Gibson

==== frame 1: What's Wrong with Preemption Points? ====

State space size depends on preemption points (PPs) chosen in advance.
- Too few PPs: poor coverage
- Too many PPs: completion is infeasible
- Can't know in advance whether resulting state space fits in CPU budget.

Most related work [1,2] hard-codes a fixed set of PPs.
- Fixed PPs miss unprotected memory accesses ("data races" [3])

Past Landslide experiments [4] let users turn PPs on/off.

*Goal*: /Automatically/ find the best set of PPs for a given CPU budget.
*Goal*: Preempt threads during data-race accesses to find new bugs.

==== frame 3: Iterative Deepening ====

*Iterative Deepening* of PPs
- Explore a minimal state space, and add more PPs until time runs out.
- Estimations decide whether to explore or defer each test.
- Completion time is controlled
  -- (at expense of predicting which PPs can be tested)

[Figure 1: trees.svg]

Advantages:
- User does not have to configure PP set; needs only supply a time limit.
  -- Good for use by students and TAs!
- Tests data races as new PPs, avoiding false positives

Limitations:
- Repeats redundant work from smaller "subset" state spaces
- Relies on imperfect estimation [5] to guide search

==== frame 4: Evaluation ====

Experiment 1: Testing student thread libraries from CMU 15-410
- 79 submissions; 6 test programs on each, 10 CPU-hours each
- Compared iterative deepening to state-of-the-art "control"
  -- Control: just 1 state space, all PPs enabled

Experiment 2: Giving Landslide to students (S'15)
- 7 groups signed up to use Landslide during P2
- Students surveyed to interpret the bugs they found
  -- 3 surveys were returned (How to encourage more?)

[ Table - results ]
--------------------------------------------------
                      | Exp. 1 | Control | Exp. 2
--------------------------------------------------
total tests run       | 474    | 474     | 63
completions (no bug)  | 367    | 407     | 21
nondeterministic bugs | 107    |  70     | 13 (distinct bugs)
data race bugs        |  37    | N/A     |  6
--------------------------------------------------

[ Figure 2: dowefindbugsfaster.svg ]

==== frame 5: Ongoing Work ====

Support for instructional OSes from other schools

Landslide can test Pintos kernels as well as Pebbles
- Continuing evaluation on 79 Pintoses from UoC and Berkeley.

*Use Pintos or another teaching OS? Let me know!* ((make sure this line stands out))
- Have students volunteer submitted code
- Plan to give Landslide to students in future semesters
- Help design test program suite to expose common races

==== frame 5: References ====

[1] Jiri Simsa. dBug, SSV 2010.
[2] Heming Cui et al. PARROT, SOSP 2013.
[3] Stefan Savage. Eraser, ACM TOCS 1997.
[4] Ben Blum. Landslide, MS thesis, CMU CS TR 12-118.
[5] Jiri Simsa, Runtime estimation, CMU PDL TR 12-113.
