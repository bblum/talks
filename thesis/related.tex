\chapter{Related Work}
\label{chap:relatedwork}
%\inspirationalquote{
%\begin{tabular}{p{0.58\textwidth}}
%To test if your paper makes a genuine contribution to its discipline,
%see if you can afford a generous tone in the "Related Work" section.
%\end{tabular}}
%{Conor McBride}

\inspirationalquote{
\begin{tabular}{p{0.51\textwidth}}
It is important to draw wisdom from many different places.
If you take it from only one place, it becomes rigid and stale.
\end{tabular}
}
{Iroh, Avatar: The Last Airbender}

%Had I a dollar for every programmer before me who thought to ``solve'' concurrency with a perfect debugging tool,
%well,
%it probably would not be quite enough to retire on.
%At any rate,
This field is built of the contributions of many a brilliant mind
trying to carve out a presentable space in an overall impossible problem,
each making their own tradeoffs along the way.
While previous chapters cited prior work as necessary in background discussions, algorithm descriptions, and so on,
this chapter aims to comprehensively tour the field,
orienting the reader's understanding of Landslide in the space of said tradeoffs.

\section{Systematic Concurrency Testing}

Equal partners in concurrency testing are the practical and the theoretical:
%the former meaning
tool implementations that target specific problem domains
%and manage practical tradeoffs,
%and help users as best one can,
%and the latter meaning
and algorithmic advances to make ever-larger state spaces computationally feasible.
%within the realm of computational feasibility.
The following two subsections discuss the most closely related prior work accordingly.
%I discuss my most closely related works split in two sections accordingly.

\subsubsection{Tools}

Systematic concurrency testing dates back to Verisoft \cite{verisoft},
the 1997 tool which first attempted to exhaustively explore the possibile ways to interleave threads.
Since then, researchers have built many tools along the same lines to test many kinds of programs.
One of the best-known SCTs is Microsoft Research's CHESS \cite{chess},
a checker for userspace C++ programs which preempts on synchronization APIs by default,
supporting compiler instrumentation to preempt on memory accesses as well,
and which pioneered the ICB search strategy discussed below.

Many checkers exist which target programs written for various different types of
concurrent execution and/or programming environments.
MaceMC \cite{macemc}, MoDist \cite{modist}, SAMC \cite{samc}, ETA \cite{dbug-retreat}, and Concuerror \cite{concuerror},
focus on distributed systems, where concurrent events are limited to message-passing and may span across multiple machines.
R4 \cite{r4} and EventRacer \cite{eventracer} check event-driven concurrent programs typical in mobile applications.
Like Landslide, SimTester \cite{simtester} is a Simics~\cite{simics}-based tool for kernel-level code,
although it focuses on interrupt nondeterminism for testing device drivers,
and is limited to injecting at most one interrupt per test run (as if under ICB with a bound of 1).
%
%Other checkers target specific programming languages' concurrency models and/or thread communication APIs.
dBug \cite{dbug-ssv}, another CMU original similar to CHESS,
tests natively-executing programs
using a dynamic library preload to insert preemption points at pthread and MPI interface boundaries.
Inspect \cite{inspect} uses a static alias analysis to instrument and preempt all memory accesses to potentially-shared data
at compile time, in addition to common synchronization APIs.
RacePRO~\cite{racepro} targets multi-process programs using system calls such as the filesystem API as preemption points
to find bugs which can corrupt persistent system resources.
SPIN \cite{spin} tests algorithms defined in the PROMELA domain-specific language,
instruments every memory access,
uses explicit state tracking rather than the stateless approach (\sect{\ref{sec:overview-stateless}}),
and specializes in verifying synchronization primitives such as RCU \cite{rcu}.
TLC \cite{tlc} checks formal models of concurrent program behaviour
written in the specification language TLA+ \cite{tlaplus},
and is arguably one of the only true concurrency {\em model checkers}
as it checks specifications separate from the programs themselves
%using explicit state tracking,
rather than attempting to exhaustively exercise every thread interleaving directly.
%
D\'{e}j\`{a} Fu \cite{dejafu} is a SCT
for the Haskell language,
whose strong type system guarantees that thread communication be confined to trusted, type-safe APIs.
%to a trusted API that implements internal synchronization to preserve type safety.
%Supporting both abstraction reduction and STM,
It instruments these interfaces (STM among them)
to check for deadlocks or nondeterministic behaviour in general,
which either may arise despite the static no-data-race guarantee.

The problem of relaxed memory nondeterminism alone has inspired the creation of several new SCTs in the past few years.
Relacy \cite{relacy}, a header-only C++ SCT library for checking synchronization primitives,
was the first to broach this field,
although requires custom annotations for non-atomic memory accesses
and
%(according to later citing papers) % cdschecker
does not fully model all possible relaxed memory behaviours.
%designed for verifying synchronization primitives,
CDSChecker \cite{cdschecker} extends DPOR with a {\em reads-from} relation
to capture most of the C++11 memory model's new behaviours.
Nidhugg \cite{nidhugg} is a SCT for TSO and PSO which instruments LLVM abstract assembly,
% i don't actually know what the difference is #overlyhonestmethods
although does not yet support the C++11 memory model.
%% actually it doesn't -- they say they take the Source DPOR worse version. im confus?
% and extends the Optimal DPOR algorithm (discussed below) to include store buffer nondeterminism.
rInspect \cite{tsopso}
%models TSO and PSO differently, using shadow threads, and
offers further heuristic state space reduction using buffer bounding (described below).
RCMC \cite{rcmc} models a ``repaired'' version of the C++11 memory model known as RC11 \cite{rc11},
and professes to achieve the best state space reduction to date.
These tools each use various heuristics to account for spin-wait loops,
ranging from delay bounding \cite{bpor} to a rigid rewrite rule,
and provide only limited support so far for read-modify-write atomics
(at best, supporting them by introducing % oops
some redundant exploration).
%
No relaxed-memory SCT has yet proposed a satisfactory model for the ``thin-air'' problem \cite{sully-thesis},
which can cause state space cycles in a way not yet well-understood and remains future work.
%and notably includes relaxed memory nondeterminism in its concurrency model.
They also identify all data races
% TODO check if ther'es not a better section reference
(under the C++ definition rather than \sect{\ref{sec:quicksand-soundness}}'s)
as bugs immediately, rather than checking them for benign or buggy outcomes.
All the tools in this paragraph are notably open-source -- an encouraging recent trend in the field.

If I might indulge by listing Landslide in its own related work section \cite{this-thesis},
I would distinguish it by its ability to find shared memory preemption points via dynamic tracing,
rather than relying on user annotations or imprecise compiler instrumentation
as other tools do.
Compared to all other tools I know of,
it implements a wider range of exponential explosion coping techniques,
some theoretical and some heuristic,
some inherited and some novel,
to help the user receive meaningful results as promptly as possible.
Its choice of a familiar pthread-like synchronization API makes it suitable for inexpert users,
and its recent extension to HTM adds support for more modern concurrency patterns as well.

\subsubsection{Algorithms}
\label{sec:related-algs}

To date a number of techniques have been proposed to mitigate exponential explosion,
the Sisyphean rock of SCT.
The notion that some interleavings of concurrent threads could lead to indistinguishable program states and be therefore redundant,
known as {\em partial order reduction} (POR),
was first proposed in \cite{partial-model-checking}
and explored in detail in \cite{partial-order-methods}.
{\em Dynamic POR} (DPOR) was later developed in \cite{dpor},
proposing to track communication events between threads on-the-fly (i.e., dynamically)
rather than to rely on imprecise static alias analyses,
and is now considered the baseline for all subsequent state space reduction approaches in SCT.
That paper includes the {\em sleep sets} extension,
which Landslide includes in its implementation.
It is a {\em sound} reduction algorithm, meaning it will never fail to test a possible program behavior, despite skipping many execution sequences.
\sect{\ref{sec:landslide-dpor}} provides a detailed walk-through of how DPOR works,
as many of this thesis's contributions build directly upon it.
%
DPOR has since been extended in several ways to achieve further reduction
and to incorporate new concurrency models.
\revision{Distributed DPOR \cite{parallel-dpor}
allows the exploration to be parallelized,
with a minimum of overhead from redundant interleavings that would ordinarily be pruned in sequential DPOR.}
Optimal DPOR \cite{optimal-dpor} extends sleep sets into the more expressive {\em wakeup trees},
which provably tests exactly one interleaving from each equivalence class,
i.e., the optimal possible reduction,
at least under the memory independence definition of equivalence.
Extending the equivalence relation itself to capture not just memory {\em address} conflicts
but also the {\em values} read and written,
SATCheck \cite{satcheck} and Maximal Causality Reduction (MCR) \cite{mcr}
use an SMT solver \cite{z3} to identify additional pruning opportunities.
Implementing parallelization, wakeup trees, or SMT-driven exploration in Landslide is left to future work.

Several other recent advances extend DPOR to new concurrency models,
beyond the shared-memory-threading model outlined in \sect{\ref{sec:landslide-dpor}}.
TransDPOR \cite{transdpor} provides extra domain-specific reduction for message-passing actor programs
by exploiting the fact that the dependency relation is transitive in the absence of shared state.
The $R^4$ algorithm \cite{r4} (corresponding to the R4 checker mentioned above)
extends DPOR to event-driven programs by separating the notion of enabled events from that of multiple threads.
TaxDC \cite{taxdc}, a taxonomy study of distributed systems concurrency bugs,
showed that for completeness distributed model checkers must incorporate many forms of nondeterminism,
including message reordering, timeouts, network disconnections, and crashes and reboots, in addition to local threads.
DPOR for TSO and PSO \cite{tsopso}
extends the concurrency model
using {\em shadow threads}, which interleave with traditional threads to represent store buffer nondeterminism,
which can expose bugs not even possible in the strong consistency model
such as discussed in \sect{\ref{sec:tm-warpzone-relaxed}}.
It also introduced a heuristic {\em buffer bounding} technique, analogous to ICB,
to mitigate the corresponding increase in state space size.
The same year, Nidhugg \cite{nidhugg} proposed a DPOR extension to account for TSO and PSO
using {\em chronological traces}.
MCR was recently extended to support relaxed memory models likewise \cite{mcr-tsopso}.
Just this year, RCMC \cite{rcmc} proposed to replace the interleaving model entirely with {\em execution graphs},
which precisely model the executions legal under the RC11 memory model,
offering further reduction still.
Somewhat analogously for HTM, this work's Chapter~\ref{chap:tm}
extended DPOR's concurrency model to include failure injection,
% not "new" -- abstraction reduction is the same as always
and proposed three reduction strategies, one sound and two heuristic,
%to make some progress up the exponential mountain.
to keep state spaces manageable.

Of course, no matter how optimal a sound reduction, there will always be programs too large to test.
To provide even partial results for state spaces that exceed the testing budget
(whether as predicted by automatic estimation \cite{estimation} or by a human's wild guess),
various heuristic exploration strategies have been proposed.
Preemption Sealing \cite{sealing} allows programmers to manually exclude preemption points
arising from trusted source code modules;
Landslide implements this as the {\tt without\_function} command (\sect{\ref{sec:landslide-pps}}).
Iterative Context Bounding (ICB) \cite{chess-icb} (\sect{\ref{sec:landslide-icb}})
orders the search space by increasing number of preemptions in each branch,
which is empirically more likely to expose bugs sooner should they exist;
BPOR \cite{bpor} extends DPOR to preserve soundness
thereunder. % what a word
Landslide implements ICB and BPOR for Chapter~\ref{chap:quicksand}'s control experiments,
although does not yet incorporate it into this thesis's own contributions
(as discussed in Chapter~\ref{chap:warpzone}).
Chapter~\ref{chap:quicksand}'s Quicksand algorithm is, effectively, another such heuristic search strategy,
focusing on preemption point subsets rather than context switch bounding.
\revision{Maple \cite{maple} proposed a heuristic metric for measuring the amount of ``concurrency coverage'',
analogous to line-by-line coverage for sequential programs,
and prioritizes testing interleavings which increase the coverage metric,
although it makes several limiting assumptions such as
% "vulnerability window", in its words
preemption locality
% this is like, TOTAL bs, completely results-oriented empiricism
% and at worst, intentionally blinding yourself to the actually difficult bugs
and independence of values read/written.}
DeMeter \cite{demeter} adapted abstraction reduction to distributed systems verification under the name Dynamic Interface Reduction,
while dBug \cite{dbug-phdthesis} applied abstraction reduction to synchronization primitives,
and I showed how it could be applied similarly to transactional memory in \sect{\ref{sec:tm-abstraction}}.
Each of these approaches is compatible (and indeed, throughout this thesis used often in concert)
with the sound reduction analyses listed above.
% mentioning PCT would disrupt the flow here and it's a dumb technique anyway so i won't bother

%%%% non-smc testing tools -- even worth talking abt?

% jepsen-io/jepsen

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Data race analysis}
\label{sec:related-data-race}

Data race analysis, originating with the lockset-only analysis of Eraser \cite{eraser},
has since grown into a mature field in its own right,
which Landslide more borrows as building blocks for its own methods rather than contributing new techniques to.
% TODO fix terminology not to conflict with dpor
Race detectors are largely distinguished by their particular flavour of the Happens-Before (HB) relation,
as discussed in \sect{\ref{sec:background-hb}}.
Djit+ \cite{djit} and FastTrack \cite{fasttrack} are among those
which soundly avoid false positives using ``Pure'' HB,
tracking Lamport-style vector clocks \cite{lamport-clocks}
for each lock and each thread to compute a global partial order on shared state accesses,
and flag any access pair not related thereby.
FastTrack optimizes Djit+'s analysis rules to remove $O(K)$ runtime factors (i.e., linear in the number of threads)
from several common read and write tracing events;
however, because $K$ is relatively small in model checking's use cases,
Landslide uses the Djit+ rules for the sake of implementation simplicity.
Meanwhile, the ``hybrid'' approach which combines DPOR-style happens-before with locksets \cite{hybriddatarace},
used in tools such as ThreadSanitizer \cite{tsan},
compute a more relaxed partial order to find more potential races in a single pass at the cost of false positives.
I called this ``Limited'' HB on account of how it excludes only those access pairs separated by blocking synchronization,
not those separated by just locks or barriers,
as compared to Pure HB.
Landslide's Limited HB implementation piggy-backs on DPOR's computed happens-before relation,
supplemented with straightforward lock-sets and heuristic treatment of lock hand-off
(often common in kernels).

Since these foundational algorithms,
many more recent works have contributed to make data-race analysis more precise, more performant,
and/or more domain-specific.
The Causally-Precedes relation \cite{predictive-dr} is a refinement of Limited HB which avoids the most common cases of false positives,
% TODO section refrance
including \sect{\ref{sec:quicksand-soundness}}'s reallocation false positives.
It could strike a middle ground in the bug-finding/verification tradeoff
between Pure and Limited HB (\sect{\ref{sec:quicksand-eval}})
that would be a welcome enhancement in Quicksand.
IFRit \cite{ifrit}
improves the performance of Pure HB using an interference analysis,
which could allow future work to avoid tracing every memory access in a simulator such as Bochs \cite{bochs} or Simics \cite{simics}.
%
DroidRacer \cite{droidracer} and CAFA \cite{cafa} find data races in Android applications,
using domain-specific heuristics (orthogonal to Quicksand's method) to reduce false positives.
DataCollider \cite{datacollider} finds data races in kernel code
by using hardware breakpoints and random sampling to achieve high performance.
% TODO: read some more recent confs for data race shit since thesprop
% probably like weak memory c.c

Although many SCTs listed in the previous section are content to report any data races as outright bugs,
RacerX \cite{racerx} showed that tools must be careful not to overwhelm users with benign warnings
they don't care about fixing.
This has motivated replay analysis
to classify data-race candidates by their impact on program behaviour
by extending single-pass data-race analysis to many thread interleavings.
It was first introduced in \cite{recordreplaydrs},
which compares the program states immediately after the access pair for differences,
preferring still to err on the side of false positives (as different program states might not necessarily lead to a failure).
RaceFuzzer \cite{racefuzzer} avoids false positives by requiring an actual failure be exhibited, as Quicksand does,
although it uses random schedule fuzzing rather than systematic testing for its concurrency coverage.
%
Portend \cite{portend} is closest in spirit to Quicksand:
it tests alternate executions based on single-pass data-race candidates to classify them in a taxonomy of likely severity,
including non-failing races which nevertheless cause nondeterministic output
in addition to obvious failures.
However, it does not
test alternate interleavings in advance of knowing any specific data races,
% TODO more speicifc section refrance
which \sect{\ref{sec:quicksand-eval}} showed is necessary to find certain bugs.
Quicksand builds on Portend's approach by introducing a feedback loop between the data-race analysis and SCT,
which results in a stronger verification property
when the test can be fully completed (\sect{\ref{sec:quicksand-soundness}}).
Portend also uses symbolic execution to test input nondeterminism as well as schedule nondeterminism,
while Quicksand remains at the mercy of manual test case design.
Future work could incorporate Portend's taxonomy to better help the user understand any non-failing data races
when the test is too large to complete,
as well as its symbolic execution to help user-provided tests achieve better coverage automatically.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Concurrency in education}

The operating systems curriculum at CMU has used the Pebbles project infrastructure
and assigned the thread library \cite{thrlib} and kernel \cite{kspec} projects
in something recognizably close to their modern forms since the Fall 2003 semester.
% the author's first semester of high school
I chose Pebbles to target with Landslide because it is closest to home, naturally.
To indulge my bias as a former member of 15-410 course staff,
I also believe that Pebbles's open-ended, design-oriented project structure is best suited
to train students to design robust concurrent code and debug it efficiently,
as it forces them to consider interactions between many different parts of their design simultaneously.
However, the difficulty of its concurrency problems (mostly having to do with thread lifecycle)
leaves little time left in the semester to cover more modern topics
such as multicore scheduling let alone transactions or relaxed memory
(all relegated to lecture material not reinforced by the assignments).

Pintos \cite{pintos} has recently emerged as the most popular educational kernel
(by count of top CS schools in the United States who teach by it);
it trades off the prevalence of its concurrency challenges to cover various OS topics more broadly,
especially advanced scheduling algorithms and filesystems.
Pintos is the stand-alone evolution of its predecessor, Nachos \cite{nachos},
which originally ran as a UNIX process with simulated device drivers.
Its popularity motivated me to extend Landslide to support it as an additional kernel architecture
(an unfortunately arduous task)
to prove Landslide's mettle beyond CMU's walls.
Xv6 \cite{xv6}, from MIT, is another major educational kernel, which is also UNIX-like and runs in QEMU,
and a natural target for model checking in future work.
Recently, Columbia introduced a new Android-focused OS course \cite{teaching-android},
which perhaps highlights the importance of related work on model-checking event-driven applications \cite{r4}.

To my knowledge, this is the first study of model checking in an educational setting,
although teaching concurrency is not itself an unstudied problem.
%% tf??
% Eytani et al.~\cite{towards-a-framework} present a promising framework for testing concurrent programs,
% which can incorporate model checking as well as static analysis, resource exhaustion, data-race analysis, and coverage analysis.
% However, it lacks an evaluation, and makes mention of its educational value only in its future work remarks.
%
%One recent study~
\cite{how-studence} surveyed how students think about
testing and debugging during a concurrent programming project,
finding that unguided, students often approach testing haphazardly,
not understanding the goal of good concurrency coverage,
and also had difficulty understanding single failing executions.
In fact, the study explicitly recommended tool support for testing many thread interleavings automatically (SCT)
and for execution traces to communicate sequences of important events (preemption traces),
which I dare say I have achieved in this thesis.
A more recent study \cite{novices-programmers}
examined in detail the students' thought process
during the diagnosis and fixing phases,
although its participants were drawn from novice-level programming classes,
and the experiment was set up with more elementary bugs like syntax and logic errors correspondingly.
Nevertheless, the authors recommended teaching debugging skills explicitly
via systematic exposure to different kinds of bugs,
which suggests future work for even advanced operating systems curricula
to offer a ``warm up'' Landslide assignment
(for example, the {\tt atomic\_*} tests from \sect{\ref{sec:education-pebbles-tests}})
that could ultimately lead to a higher solve rate on Landslide's bug reports during P2 (\sect{\ref{sec:education-eval-bugs-cmu}}).

%% this 2nd paper... not freely available.......
%while Kolikant \cite{learning-concurrency} investigates how students form cognitive patterns about concurrent programming that could either aid or stunt their reasoning.
%Both of these studies could help optimize Landslide's bug reports for clarity and student enlightenment.
Willgrind \cite{willgrind} is a tool recently developed at Virginia Tech
that targets a fork-join parallelism project
and checks for memory errors (using the Valgrind \cite{valgrind} framework)
as well as deadlocks, assertion failures, and data races, similarly to Landslide,
although unlike Landslide, its thread interleaving coverage is as yet limited to stress testing.
Its GUI-based debugging output is perhaps more friendly than Landslide's HTML preemption traces,
and its user survey found that students appreciated detailed debugging info especially for deadlocks
(future work for Landslide),
but also that students had little patience for even a 5-minute stress test
when no assurance against false negatives could be provided.
This suggests motivating students with Landslide's verification guarantee,
% TODO fix this explanation its too roundabout
although it is tricky to avoid accientally encouraging % oops
them to limit possible interleavings by just using one global lock for everything,
which is counter to 15-410's educational goals.

\section{Transactional memory}

Transactional memory (TM), first introduced in 1993 \cite{transactional-memory},
% TODO expand this to talk about some stm implementations?
has received renewed attention in recent years with the announcement of Intel's Haswell architecture \cite{htm-haswell},
which supports hardware transactions (HTM) using new x86 instructions.
Since then, many studies have evaluated the increased performance it offers over traditional locking and/or STM
\cite{htm-experience,htm-performance,tm-benchmark-cmu}.
%
HTM's performance comes at an increased cost in complexity to the programmer,
who must avoid system calls or transaction nesting, respect the CPU cache capacity,
and consider retry loops for spurious failure.
SI-TM \cite{si-tm} introduces techniques for reducing HTM's abort rates for performance's sake,
but without eliminating them altogether, any full verification must still consider them possible anywhere.
For programmers who wish to avoid such concerns,
the simpler STM programming model remains relevant.
One recent work \cite{hybrid-htm-stm} enhances STM transactions to nest with HTM ones,
while another \cite{stm-relaxed-memory} adds support for relaxed memory models.
Meanwhile, two recent papers \cite{relaxed-transactions-pldi,relaxed-transactions-popl}
have proposed formal models of HTM's execution semantics under relaxed memory likewise.
Such extensions come with the challenge of even more complicated behavioural semantics
for SCT to accurately model and verify in future work.

Testing approaches for transactional programs are sparsely represented in the literature so far.
Although several related works \cite{tm-correctness,tm-completeness,specifying-verifying-tm}
are building up to formal proofs of the correctness of underlying TM {\em implementations},
Landslide is the first I know of to verify client programs thereof.
McRT STM \cite{mc-tm-with-spin} uses SPIN \cite{spin} to model check an STM implementation
up to 2 threads running 1 transaction each with up to 3 memory accesses.
This kind of verification,
analogous to \sect{\ref{sec:education-pebbles-tests}}'s {\tt mutex\_test},
is an important stepping stone for trusting the results Landslide will provide.
STAMP \cite{stamp} is a benchmark suite transactional programs,
implemented using the OpenTM interface \cite{opentm},
used by many papers in the field to evaluate the performance of both STM and HTM implementations alike.
As discussed in \sect{\ref{sec:tm-eval-exp-setup}},
it focuses more on performance than on interesting concurrency properties.
% stamp bugfixes - https://github.com/mfs409/transmem/blob/master/benchmarks/stamp_c/VERSIONS.
% stamp need not be considered harmful put here
Even so, the more recent Stampede suite \cite{scalable-tm} argues that STAMP's benchmarks
were constructed under a programming model poorly-suited to fully take advantage of HTM's performance,
and that scalable HTM programs must
minimize incidental conflicts and handle aborts more flexibly than with blind retry loops.
The programming complexity needed to achieve these goals calls,
of course,
for correspondingly advanced verification approaches such as Landslide.
Finally, TxRace \cite{txrace}
tests non-transactional programs for data races
by inserting HTM calls via compiler instrumentation,
% they don't actually seem to check the failure reason lmao
% i'll phrase this in a slightly weasel way to point out that's what they *should* be doing
% and that in turn motivates the stuff i did
% even tho they use a diff way to filter them post hoc (well, they'd still need to, bc of false sharing)
relying on conflict aborts to point out access pairs that would be unsafe in the original program.
This citation arguably belongs in \sect{\ref{sec:related-data-race}} as well;
I include it here to highlight the importance of Landslide's
ability to distinguish different abort reasons (\sect{\ref{sec:txn-abort-codes}}).

An article from relatively early in the timeline of TSX \cite{htm-subtleties}
warns of several false equivalence pitfalls when converting conventionally-locking code to use transactions,
although these pitfalls depend on multiple existing locks used locally and disjointly,
so this does not invalidate the equivalence proved in \sect{\ref{sec:tm-design-locks}}.
Rather, Landslide could be used to ensure that freshly-converted transactional code avoids the warned-of pitfalls.
{\em Learning from Mistakes} \cite{learning-from-mistakes},
a survey of the characteristics of many types of concurrency bugs,
found that TM could potentially fix some, but not all, of the studied bugs,
while in other cases it must be combined with other concurrency primitives to be fully correct.
A subsequent paper \cite{applying-tm-bugs}
found a majority of bugs in their study to be easily fixable with hand-written transactions,
while others remained out of scope due to blocking {\tt cond\_wait()} operations and the like;
more recently, the tool BugTM \cite{bugtm} aims to deploy such repairs in production code fully automatically.
However, these studies all optimize for empirical correctness at best,
as well as maintaining good performance,
which motivates the use of tools like Landslide to ensure these rewrites
are actually correct,
% oops
rather than merely shrinking the necessary preemption window required to expose them.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Other concurrency verification approaches}

%It should come as no surprise that
Naturally, many avenues of research towards writing correct programs
have been explored
apart from just executing them a bunch of times to check all the interleavings.
Though not as directly related as the works referenced above,
this section explores such approaches,
ranging from expressing safety guarantees in a language's type system
to checking, proving, and/or enforcing execution properties post-hoc.

\subsubsection{Programming language design}

While C's extremely rudimentary type system allows the compiler to statically check programs
for properties such as not accidentally dereferencing raw integer values as if they were pointers,
more advanced programming languages may make guarantees about concurrent execution.
%
Erlang \cite{erlang}, an early concurrent functional language,
introduced the actor model for concurrency,
in which threads share no state and must communicate only by message-passing.
%One might say this statically guarantees freedom from data races;
%from another point of view, data races do not even exist as a concept in this programming model.
While this statically guarantees the absence of data races,
programs may still execute nondeterministically,
so concurrency bugs, especially deadlocks, are not ruled out.
Concuerror \cite{concuerror}, discussed above, is a SCT tool for Erlang programs.
%
Haskell \cite{haskell} offers a more sophisticated interface to concurrency:
threads may reference the same objects and even update shared references
using monads that encapsulate mutation,
but at the (garbage-collected) execution level all data is immutable once created,
which preserves type soundness and data-race freedom.
The aforementioned D\'{e}j\`{a} Fu \cite{dejafu} checks concurrent Haskell programs.
%
Rust \cite{rust-language}
presents a type system with more explicit memory management,
in-place mutation,
and mutable references
to appear familiar and approachable to those already versed in C++.
It proposes a borrow-check analysis to ensure memory and type safety
despite mutable references,
and a trait system to ensure no shared state between threads by default.
Its concurrency libraries then offer interfaces which relax this restriction,
allowing threads even to simultaneously reference shared mutable state,
using the type system to enforce sound locking discipline across such accesses,
again preserving type soundness and data-race freedom%
\footnote{The author themself contributed the original design for this latter feature.}.
I know of no existing SCT for Rust as of yet.
%
The Relaxed Memory Calculus \cite{sully-thesis}
proposes to extend C++ with annotations for weak memory atomics,
which allows for static formal analysis of memory access reorderings.
Although not ruling out data races,
this approach is an important step towards compilers
which can statically reason about program execution under more advanced concurrency models.
%
Finally, LVish \cite{lvish}
features a type system that enforces deterministic behaviour by construction,
using shared state called LVars which allow writes only in ways that update order is not observable.
This renders thread interleavings entirely irrelevant,
obviating any need for runtime verification,
but at the cost of a more restrictive programming model.

\subsubsection{Deterministic multithreading}

Coming at nondeterminsm from the opposite angle as SCT,
which aims to push the frontier of testing coverage to expand as many interleavings as possible,
is deterministic multithreading,
which reduces the number of interleavings possible to begin with enough that said frontier can reach it more easily.
Unlike LVish, described above, these systems provide deterministic execution
even for the familiar, C-like, shared-state multithreading programming model.
Kendo~\cite{kendo} and CoreDet \cite{coredet} were among the first systems to implement this,
but were limited in which sources of nondeterminism they could control
and suffered high performance overhead.
DThreads \cite{dthreads} then extended the scope of determinization to include data races,
while Peregrine \cite{peregrine}
improved performance by using record-and-replay to compute a set of possible safe schedules.
%
Parrot \cite{parrot} later integrated with the aforementioned dBug \cite{dbug-ssv}
to offer a partially-determinizing runtime scheduler
that offered near-baseline performance by allowing the programmer to
manually annotate speed-critical nondeterministic sections
and then check the resulting state spaces using dBug's SCT as normal.
%
Most recently, Sofritas \cite{sofritas}
proposed the Ordering-Free Region execution model
which restricts nondeterminism to only order-enforcing operations such as blocking,
and automatically suggests refinement annotations to the programmer
when that would be too aggressive for the intended behaviour.
These determinizing runtimes serve a different purpose than SCT:
they seek to preserve the stability of existing code already running in production,
whether or not concurrency bugs may exist,
while this thesis aims to eradicate as many such bugs as possible beforehand.
As Parrot demonstrated, the two approaches are compatible in cases where either extreme be infeasible.

\subsubsection{Symbolic execution}

Analogous to SCT,
which seeks good coverage of possible thread execution paths under schedule nondeterminism,
another popular testing approach is symbolic execution \cite{symbolic-execution},
which seeks good coverage of possible flow control paths under input nondeterminism.
Symbolic executors
abstract a program's variables and use constraint solvers such as Z3 \cite{z3}
to work backwards and synthesize combinations of test inputs which can lead to a failure.
%
KLEE \cite{klee}, one well-known and open-source implementation,
offers over 90\% code coverage on average across many tests,
often outdoing that achieved by programmers' own hand-written tests.
%
Later, Contessa \cite{contessa} extended symbolic execution to include concurrency nondeterminism as well,
by using a DPOR-like analysis on individual execution traces
then including reordering possibilities in its SMT constraints.
%rather than using explicit scheduling as SCT does.
This simultaneously exercises both input and schedule nondeterminism,
%and achieves good bug-finding performance in practice,
but does not provide the same verification guarantees as repeated DPOR iterations with explicit scheduling.
Exploring both kinds of state space at once thoroughly enough to provide strong verification
is undoubtedly subject to further state space explosion,
and remains future work.
%
Symbiosis \cite{symbiosis} starts from the known root cause of an existing failure
and uses symbolic execution to synthesize a schedule to reproduce it,
then further searches for a non-failing schedule and compares them to produce
a minimum sequence of events necessary for the failure.
This approach skips the initial verification step entirely,
but greatly reduces the diagnosis effort required of the user,
which was a common complaint about Landslide's preemption traces.

\subsubsection{Formal verification}

seL4 \cite{sel4} is a microkernel fully designed and specified in Haskell and translated into C.
Its proofs guarantee not only standard security properties such as process isolation and bounded interrupt latency,
but also that the C code faithfully implements the specification.
It addresses concurrency by enabling system interrupts only at carefully-chosen code points,
and proving bounded runtime besides to ensure good preemptibility.
%and that the executable binary is correctly compiled from the C.
This degree of verification must however come at a cost:
seL4's authors reported over 2 person-years of development effort, with the majority spent on the Haskell specification.
%
CertiKOS \cite{certikos} extends this approach to include full concurrency and fine-grained locking
in the scope of verification,
using a proof in the Coq proof assistant
\cite{coq}
that also took 2 person-years to complete.
Its safety properties hold under all possible interleavings,
and include data-race freedom as well as standard sequential properties
such as no null dereference and no buffer or integer overflow,
although it stops short of reasoning about relaxed memory orderings or the TLB cache.
Many programmers would find a verification cost measured in person-years far too prohibitive,
while others might argue that for safety-critical kernel code you can't afford {\em not} to verify so thoroughly.
%
More recently, Hyperkernel \cite{hyperkernel} extended the xv6 educational kernel \cite{xv6}
to allow for partial, case-by-case verification of system call behaviour using state-machine specification in Python
checked by an SMT solver.
To limit verification complexity, it assumes not only uniprocessor execution
but also that interrupts be perpetually disabled,
taking concurrency entirely out of the equation to allow for greater extensibility
and lessen the programmer's verification burden.
%
Heroic as such end-to-end formal verification projects are,
this thesis finds that trading off thoroughness for accessibility is also acceptable
if it means helping more users overall.
Future work could check preemptive and/or multiprocessor kernels
%by combining Hyperkernel's approach,
by first checking safety properties in the absence of concurrency,
then checking with Landslide that concurrency introduces no new program behaviour not already verified,
to provide a less formal, but still hopefully useful, verification guarantee.
%
\revision{Lastly, CompCertTSO \cite{compcerttso} extends the CompCert verified compiler \cite{compcert}
to capture x86's relaxed memory semantics,
guaranteeing that a program written in the ClightTSO subset of C
is translated accurately to assembly with the same behaviour.
Like CertiKOS, its implementation is verified in Coq.
Although Landslide checks programs directly at the executable level,
blind to the source code the programmer personally wrote,
extending that pipeline with a certifying compiler would
improve the overall verification,
ensuring that Landslide was actually checking the behaviour the programmer intended.}
