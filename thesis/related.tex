\chapter{Related Work}
\label{chap:relatedwork}
%\inspirationalquote{
%\begin{tabular}{p{0.58\textwidth}}
%To test if your paper makes a genuine contribution to its discipline,
%see if you can afford a generous tone in the "Related Work" section.
%\end{tabular}}
%{Conor McBride}

\inspirationalquote{
\begin{tabular}{p{0.51\textwidth}}
It is important to draw wisdom from many different places.
If you take it from only one place, it becomes rigid and stale.
\end{tabular}
}
{Iroh, Avatar: The Last Airbender}

%Had I a dollar for every programmer before me who thought to ``solve'' concurrency with a perfect debugging tool,
%well,
%it probably would not be quite enough to retire on.
%At any rate,
This field is built of the contributions of many a brilliant mind
trying to carve out a presentable space in an overall impossible problem,
each making their own tradeoffs along the way.
While previous chapters cited prior work as necessary in background discussions, algorithm descriptions, and so on,
this chapter aims to comprehensively tour the field,
orienting the reader's understanding of Landslide in the space of said tradeoffs.

\section{Systematic Concurrency Testing}

Equal partners in concurrency testing are the practical and the theoretical:
the former meaning tool implementations that target specific problem domains and help users as best one can,
and the latter meaning algorithmic advances to bring ever-larger state spaces within the realm of computational feasibility.
I discuss my most closely related works split in two sections accordingly.

\subsubsection{Tools}

Systematic concurrency testing dates back to Verisoft \cite{verisoft},
the 1997 tool which first attempted to exhaustively explore the possibile ways to interleave threads.
Since then, researchers have built many tools along the same lines to test many kinds of programs.
One of the best-known SCTs is Microsoft Research's CHESS \cite{chess},
a checker for userspace C++ programs which preempts on synchronization APIs by default,
supporting compiler instrumentation to preempt on memory accesses as well,
and which pioneered the ICB search strategy discussed below.

Many checkers exist which target programs written for various different types of concurrent environments.
MaceMC \cite{macemc}, MoDist \cite{modist}, SAMC \cite{samc}, ETA \cite{dbug-retreat}, and Concuerror \cite{concuerror},
focus on distributed systems, where concurrent events are limited to message-passing and may span across multiple machines.
R4 \cite{r4} and EventRacer \cite{eventracer} check event-driven concurrent programs typical in mobile applications.
dBug \cite{dbug-ssv}, another CMU original similar to CHESS,
integrates with Parrot \cite{parrot}, a determinizing runtime scheduler,
to approach state space reduction from a different angle by limiting which thread interleavings are possible to begin with.

Other checkers target specific programming languages' concurrency models and/or thread communication APIs.
Inspect \cite{inspect} uses a static alias analysis to instrument and preempt all memory accesses to potentially-shared data.
SPIN \cite{spin} tests algorithms defined in the PROMELA domain-specific language,
instruments every memory access, and
specializes in verifying synchronization primitives such as RCU \cite{rcu}.
%
D\'{e}j\`{a} Fu \cite{dejafu} is a SCT
for the Haskell language,
whose strong type system guarantees that thread communication be confined to trusted, type-safe APIs.
%to a trusted API that implements internal synchronization to preserve type safety.
%Supporting both abstraction reduction and STM,
It instruments these interfaces (STM among them)
to check for deadlocks or nondeterministic behaviour in general,
which either may arise despite the static no-data-race guarantee.
The Rust language \cite{rust-language}
provides a more C++-like type system that also
%, which I contributed to personally,
achieves the same guarantee, although I know of no existing SCT for it yet.

The problem of relaxed memory nondeterminism alone has inspired the creation of several new SCTs in the past few years.
Relacy \cite{relacy}, a header-only C++ SCT library for checking synchronization primitives,
was the first to broach this field,
although requires custom annotations for non-atomic memory accesses
and
%(according to later citing papers) % cdschecker
does not fully model all possible relaxed memory behaviours.
%designed for verifying synchronization primitives,
CDSChecker \cite{cdschecker} extends DPOR with a {\em reads-from} relation
to capture most of the C++11 memory model's new behaviours.
Nidhugg \cite{nidhugg} is a SCT for TSO and PSO which instruments LLVM abstract assembly,
% i don't actually know what the difference is #overlyhonestmethods
although does not yet support the C++11 memory model.
%% actually it doesn't -- they say they take the Source DPOR worse version. im confus?
% and extends the Optimal DPOR algorithm (discussed below) to include store buffer nondeterminism.
rInspect \cite{tsopso}
%models TSO and PSO differently, using shadow threads, and
offers further heuristic state space reduction using buffer bounding (described below).
RCMC \cite{rcmc} models a ``repaired'' version of the C++11 memory model known as RC11 \cite{rc11},
and professes to achieve the best state space reduction to date.
These tools each use various heuristics to account for spin-wait loops,
ranging from delay bounding \cite{bpor} to a rigid rewrite rule,
and provide only limited support so far for read-modify-write atomics
(at best, supporting them by introducing % oops
some redundant exploration).
%
No relaxed-memory SCT has yet proposed a satisfactory model for the ``thin-air'' problem \cite{sully-thesis},
which can cause state space cycles in a way not yet well-understood and remains future work.
%and notably includes relaxed memory nondeterminism in its concurrency model.
They also identify all data races
% TODO check if ther'es not a better section reference
(by the C++ definition rather than \sect{\ref{sec:quicksand-soundness}}'s)
as bugs immediately, rather than checking them for benign or buggy outcomes.
All the tools in this paragraph are notably open-source -- an encouraging recent trend in the field.


If I might indulge by listing Landslide in its own related work section \cite{this-thesis},
I would distinguish it by its ability to find shared memory preemption points via dynamic tracing,
rather than relying on user annotations or imprecise compiler instrumentation
as other tools do.
Compared to all other tools I know of,
it implements a wider range of exponential explosion coping techniques,
some theoretical and some heuristic,
some inherited and some novel,
to help the user receive meaningful results as promptly as possible.
Its choice of a familiar pthread-like synchronization API makes it suitable for inexpert users,
and its recent extension to HTM adds support for more modern concurrency patterns as well.

% TODO
% Surveying concurrency bug detectors based on types of detected bugs - idk what this is, from an email

\subsubsection{Algorithms}

To date a number of techniques have been proposed to mitigate exponential explosion,
the Sisyphean rock of SCT.
The notion that some interleavings of concurrent threads could lead to indistinguishable program states and be therefore redundant,
known as {\em partial order reduction} (POR),
was first proposed in \cite{partial-model-checking}
and explored in detail in \cite{partial-order-methods}.
{\em Dynamic POR} (DPOR) was later developed in \cite{dpor},
proposing to track communication events between threads on-the-fly (i.e., dynamically)
rather than to rely on imprecise static alias analyses,
and is now considered the baseline for all subsequent state space reduction approaches in SCT.
That paper includes the {\em sleep sets} extension,
which Landslide includes in its implementation.
It is a {\em sound} reduction algorithm, meaning it will never fail to test a possible program behavior, despite skipping many execution sequences.
\sect{\ref{sec:landslide-dpor}} provides a detailed walk-through of how DPOR works,
as many of this thesis's contributions build directly upon it.

DPOR has since been extended in several ways to achieve further reduction
and to incorporate new concurrency models.
Optimal DPOR \cite{optimal-dpor} extends sleep sets into the more expressive {\em wakeup trees},
which provably tests exactly one interleaving from each equivalence class,
i.e., the optimal possible reduction,
at least under the memory independence definition of equivalence.
Extending the equivalence relation itself to capture not just memory {\em address} conflicts
but also the {\em values} read and written,
SATCheck \cite{satcheck} and Maximal Causality Reduction (MCR) \cite{mcr}
use an SMT solver \cite{z3} to identify additional pruning opportunities.
Implementing wakeup trees or SMT-driven exploration in Landslide is left to future work.

Several other recent advances extend DPOR to new concurrency models,
beyond the shared-memory-threading model outlined in \sect{\ref{sec:landslide-dpor}}.
TransDPOR \cite{transdpor} provides extra domain-specific reduction for message-passing actor programs
by exploiting the fact that the dependency relation is transitive in the absence of shared state.
The $R^4$ algorithm \cite{r4} (corresponding to the R4 checker mentioned above)
extends DPOR to event-driven programs by separating the notion of enabled events from that of multiple threads.
DPOR for TSO and PSO \cite{tsopso}
extends the concurrency model
using {\em shadow threads}, which interleave with traditional threads to represent store buffer nondeterminism,
which can expose bugs not even possible in the strong consistency model
such as discussed in \sect{\ref{sec:tm-warpzone-relaxed}}.
It also introduced a heuristic {\em buffer bounding} technique, analogous to ICB,
to mitigate the corresponding increase in state space size.
The same year, Nidhugg \cite{nidhugg} proposed a DPOR extension to account for TSO and PSO
using {\em chronological traces}.
MCR was recently extended to support relaxed memory models likewise \cite{mcr-tsopso}.
Just this year, RCMC \cite{rcmc} proposed to replace the interleaving model entirely with {\em execution graphs},
which precisely model the executions legal under the RC11 memory model,
offering further reduction still.
Somewhat analogously for HTM, this work's Chapter~\ref{chap:tm}
extended DPOR's concurrency model to include failure injection,
% not "new" -- abstraction reduction is the same as always
and proposed three reduction strategies, one sound and two heuristic,
%to make some progress up the exponential mountain.
to keep state spaces manageable.

Of course, no matter how optimal a sound reduction, there will always be programs too large to test.
To provide even partial results for state spaces that exceed the testing budget
(whether as predicted by automatic estimation \cite{estimation} or by a human's wild guess),
various heuristic exploration strategies have been proposed.
Preemption Sealing \cite{sealing} allows programmers to manually exclude preemption points
arising from trusted source code modules;
Landslide implements this as the {\tt without\_function} command (\sect{\ref{sec:landslide-pps}}).
Iterative Context Bounding (ICB) \cite{chess-icb} (\sect{\ref{sec:landslide-icb}})
orders the search space by increasing number of preemptions in each branch,
which is more likely to expose bugs sooner should they exist;
BPOR \cite{bpor} extends DPOR to preserve soundness
thereunder. % what a word
Landslide implements ICB and BPOR for Chapter~\ref{chap:quicksand}'s control experiments,
although does not yet incorporate it into this thesis's own contributions.
Chapter~\ref{chap:quicksand}'s Quicksand algorithm is, effectively, another such heuristic search strategy,
focusing on preemption point subsets rather than context switch bounding.
Each of these approaches is compatible (and indeed, throughout this thesis used often in concert)
with the sound reduction analyses listed above.
% mentioning PCT would disrupt the flow here and it's a dumb technique anyway so i won't bother

%%%% non-smc testing tools -- even worth talking abt?

% jepsen-io/jepsen

\section{Data race analysis}

Data race analysis, originating with the lockset-only analysis of Eraser \cite{eraser},
has since grown into a mature field in its own right,
which Landslide more borrows as building blocks for its own methods rather than contributing new techniques to.
% TODO fix terminology not to conflict with dpor
Race detectors are largely distinguished by their particular flavour of the Happens-Before (HB) relation,
as discussed in \sect{\ref{sec:background-hb}}.
Djit+ \cite{djit} and FastTrack \cite{fasttrack} are among those
which soundly avoid false positives using ``Pure'' HB,
tracking Lamport-style vector clocks \cite{lamport-clocks}
for each lock and each thread to compute a global partial order on shared state accesses,
and flag any access pair not related thereby.
FastTrack optimizes Djit+'s analysis rules to remove $O(K)$ runtime factors (i.e., linear in the number of threads)
from several common read and write tracing events;
however, because $K$ is relatively small in model checking's use cases,
Landslide uses the Djit+ rules for the sake of implementation simplicity.
Meanwhile, the ``hybrid'' approach which combines DPOR-style happens-before with locksets \cite{hybriddatarace},
used in tools such as ThreadSanitizer \cite{tsan},
compute a more relaxed partial order to find more potential races in a single pass at the cost of false positives.
I called this ``Limited'' HB on account of how it excludes only those access pairs separated by blocking synchronization,
not those separated by just locks or barriers,
as compared to Pure HB.
Landslide's Limited HB implementation piggy-backs on DPOR's computed happens-before relation,
supplemented with straightforward lock-sets and heuristic treatment of lock hand-off
(often common in kernels).

Since these foundational algorithms,
many more recent works have contributed to make data-race analysis more precise, more performant,
and/or more domain-specific.
The Causally-Precedes relation \cite{predictive-dr} is a refinement of Limited HB which avoids the most common cases of false positives,
% TODO section refrance
including \sect{\ref{sec:quicksand-soundness}}'s reallocation false positives.
It could strike a middle ground in the bug-finding/verification tradeoff
between Pure and Limited HB (\sect{\ref{sec:quicksand-eval}})
that would be a welcome enhancement in Quicksand.
IFRit \cite{ifrit}
improves the performance of Pure HB using an interference analysis,
which could allow future work to avoid tracing every memory access in a simulator such as Bochs \cite{bochs} or Simics \cite{simics}.
%
DroidRacer \cite{droidracer} and CAFA \cite{cafa} find data races in Android applications,
using domain-specific heuristics (orthogonal to Quicksand's method) to reduce false positives.
DataCollider \cite{datacollider} finds data races in kernel code
by using hardware breakpoints and random sampling to achieve high performance.
% TODO: read some more recent confs for data race shit since thesprop
% probably like weak memory c.c

Although many SCTs listed in the previous section are content to report any data races as outright bugs,
RacerX \cite{racerx} showed that tools must be careful not to overwhelm users with benign warnings
they don't care about fixing.
This has motivated replay analysis
to classify data-race candidates by their impact on program behaviour
by extending single-pass data-race analysis to many thread interleavings.
It was first introduced in \cite{recordreplaydrs},
which compares the program states immediately after the access pair for differences,
preferring still to err on the side of false positives (as different program states might not necessarily lead to a failure).
RaceFuzzer \cite{racefuzzer} avoids false positives by requiring an actual failure be exhibited, as Quicksand does,
although it uses random schedule fuzzing rather than systematic testing for its concurrency coverage.
%
Portend \cite{portend} is closest in spirit to Quicksand:
it tests alternate executions based on single-pass data-race candidates to classify them in a taxonomy of likely severity,
including non-failing races which nevertheless cause nondeterministic output
in addition to obvious failures.
However, it does not
test alternate interleavings in advance of knowing any specific data races,
% TODO more speicifc section refrance
which \sect{\ref{sec:quicksand-eval}} showed is necessary to find certain bugs.
Quicksand builds on Portend's approach by introducing a feedback loop between the data-race analysis and SCT,
which results in a stronger verification property
when the test can be fully completed (\sect{\ref{sec:quicksand-soundness}}).
Portend also uses symbolic execution to test input nondeterminism as well as schedule nondeterminism,
while Quicksand remains at the mercy of manual test case design.
Future work could incorporate Portend's taxonomy to better help the user understand any non-failing data races
when the test is too large to complete,
as well as its symbolic execution to help user-provided tests achieve better coverage automatically.

\section{Concurrency in Education}

The operating systems curriculum at CMU has used the Pebbles project infrastructure
and assigned the thread library \cite{thrlib} and kernel \cite{kspec} projects
in something recognizably close to their modern forms since the Fall 2003 semester.
% the author's first semester of high school
I chose Pebbles to target with Landslide because it is closest to home, naturally.
To indulge my bias as a former member of 15-410 course staff,
I also believe that Pebbles's open-ended, design-oriented project structure is best suited
to train students to design robust concurrent code and debug it efficiently,
as it forces them to consider interactions between many different parts of their design simultaneously.
However, the difficulty of its concurrency problems (mostly having to do with thread lifecycle)
leaves little time left in the semester to cover more modern topics
such as multicore scheduling let alone transactions or relaxed memory
(all relegated to lecture material not reinforced by the assignments).

Pintos \cite{pintos} has recently emerged as the most popular educational kernel
(by count of top CS schools in the USA who teach by it);
it trades off the prevalence of its concurrency challenges to cover various OS topics more broadly,
especially advanced scheduling algorithms and filesystems.
Pintos is the stand-alone evolution of its predecessor, Nachos \cite{nachos},
which originally ran as a UNIX process with simulated device drivers.
Its popularity motivated me to extend Landslide to support it as an additional kernel architecture
(an unfortunately arduous task)
to prove Landslide's mettle beyond CMU's walls.
Xv6 \cite{xv6}, from MIT, is another major educational kernel, which is also UNIX-like and runs in QEMU,
and a natural target for model checking in future work.
Recently, Columbia introduced a new Android-focused OS course \cite{teaching-android},
which perhaps highlights the importance of related work on model-checking event-driven applications \cite{r4}.

To my knowledge, this is the first study of model checking in an educational setting,
although teaching concurrency is not itself an unstudied problem.
%% tf??
% Eytani et al.~\cite{towards-a-framework} present a promising framework for testing concurrent programs,
% which can incorporate model checking as well as static analysis, resource exhaustion, data-race analysis, and coverage analysis.
% However, it lacks an evaluation, and makes mention of its educational value only in its future work remarks.
%
%One recent study~
\cite{how-studence} surveyed how students think about
testing and debugging during a concurrent programming project,
finding that unguided, students often approach testing haphazardly,
not understanding the goal of good concurrency coverage,
and also had difficulty understanding single failing executions.
In fact, the study explicitly recommended tool support for testing many thread interleavings automatically (SCT)
and for execution traces to communicate sequences of important events (preemption traces),
which I dare say I have achieved in this thesis.
For future work, I would also recommend studying in more detail the students' thought process
between receiving a buggy preemption trace and fixing their bug,
which could perhaps inspire a ``warm up'' Landslide assignment to teach debugging skills explicitly
and ultimately lead to a higher solve rate on Landslide's bug reports during P2 (\sect{\ref{sec:education-eval-bugs-cmu}}).
%% this 2nd paper... not freely available.......
%while Kolikant \cite{learning-concurrency} investigates how students form cognitive patterns about concurrent programming that could either aid or stunt their reasoning.
%Both of these studies could help optimize Landslide's bug reports for clarity and student enlightenment.
Willgrind \cite{willgrind} is a tool recently developed at Virginia Tech
that targets a fork-join parallelism project
and checks for memory errors (using the Valgrind \cite{valgrind} framework)
as well as deadlocks, assertion failures, and data races, similarly to Landslide,
although unlike Landslide, its thread interleaving coverage is as yet limited to stress testing.
Its GUI-based debugging output is perhaps more friendly than Landslide's HTML preemption traces,
and its user survey found that students appreciated detailed debugging info especially for deadlocks
(future work for Landslide),
but also that students had little patience for even a 5-minute stress test
when no assurance against false negatives could be provided.
This suggests motivating students with Landslide's verification guarantee,
although it is tricky to avoid accientally encouraging % oops
students to limit possible interleavings by just using one global lock for everything,
which is counter to 15-410's educational goals.

\section{Transactional memory}

Transactional memory (TM), first introduced in 1993 \cite{transactional-memory},
% TODO expand this to talk about some stm implementations?
has received renewed attention in recent years with the announcement of Intel's Haswell architecture \cite{htm-haswell},
which supports hardware transactions (HTM) using new x86 instructions.
Since then, many studies have evaluated the increased performance it offers over traditional locking and/or STM
\cite{tm-benchmark-cmu}. % TODO more
%
HTM's performance comes at an increased cost in complexity to the programmer,
who must avoid system calls or transaction nesting, respect the CPU cache capacity,
and consider retry loops for spurious failure.
SI-TM \cite{si-tm} introduces techniques for reducing HTM's abort rates for performance's sake,
but without eliminating them altogether, any full verification must still consider them possible anywhere.
For programmers who wish to avoid such concerns,
the simpler STM programming model remains relevant.
One recent work \cite{hybrid-htm-stm} enhances STM transactions to nest with HTM ones,
while another \cite{stm-relaxed-memory} adds support for relaxed memory models.
Such extensions come with the challenge of even more complicated behavioural semantics
for SCT to accurately model and verify in future work.

Testing approaches for transactional programs are sparsely represented in the literature so far.
Although several related works \cite{tm-correctness,tm-completeness,specifying-verifying-tm}
are building up to formal proofs of the correctness of underlying TM {\em implementations},
Landslide is the first I know of to verify client programs thereof.
McRT STM \cite{mc-tm-with-spin} uses SPIN \cite{spin} to model check an STM implementation
up to 2 threads running 1 transaction each with up to 3 memory accesses.
This kind of verification,
analogous to \sect{\ref{sec:education-pebbles-tests}}'s {\tt mutex\_test},
is an important stepping stone for trusting the results Landslide will provide.
% TODO more tm impl verifs

% TODO: find a home for this sentence
{\em Learning from Mistakes} \cite{learning-from-mistakes},
a survey of the characteristics of many types of concurrency bugs,
found that TM could potentially fix some, but not all, of the studied bugs,
while in other cases it must be combined with other concurrency primitives to be fully correct,
motivating the use of SCT to verify such transactional code.





% cchtm - the intel ismm17 paper on nvram htm
% stamp (mb just cite from txn section)

% transactions in relaxed mem architecutres - popl 18

% pldi 2018
% The Semantics of Transactions and Weak Memory in x86, Power, ARM, and C++
