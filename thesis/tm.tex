\chapter{Transactions}
\label{chap:tm}

%\inspirationalquote{This paper presents earth-shaking work that deserves to be read by the entire computer science community.
%To that end, have you considered putting it in a thesis?}
%{Anonymous SIGBOVIK reviewer}

\inspirationalquote{
\begin{tabular}{p{0.47\textwidth}}
One can only build a sand castle where the sand is wet. \\
But where the sand is wet, the tide comes. \\
Yet we still build sand castles.
\end{tabular}}
{Yuri, Doki Doki Literature Club}

Transactional memory (TM) is a concurrency primitive
by which programmers may attempt an arbitrary sequence of shared memory accesses,
which will either be committed (made visible to other processors/threads) atomically,
such that no intermediate state modification is ever visible,
or in the case of a conflict which would prevent such,
discarded with an error code returned to allow the programmer to write a synchronized backup path.
%
Transactional memory specifications typically have three API functions, abstractly speaking:
\begin{itemize}
	\item  {\sf begin} begins a transaction,
		staging any subsequent shared memory accesses in some temporary thread-local storage,
		and checking for conflicts with the accesses of any other threads or CPUs.
		If the transaction is unsuccessful, as described below,
		{\sf begin} instead returns an error code
		indicating the programmer should fall back to some other, possibly slower, synchronization method.
	\item {\sf end} ends a transaction,
		attempting to commit all staged accesses to the shared memory atomically with respect to
		reads or writes from other concurrently-executing code.
		If any of those accesses conflict (i.e., read/write or write/write)
		with any other access to the same memory since the transaction started,
		they are instead discarded and execution state reverts to the {\sf begin} with an error code as described above.
	\item {\sf abort} explicitly aborts a transaction,
		regardless of any memory conflicts,
		discarding changes and reverting execution as described above.
		Some implementations allow an arbitrary abort code to be specified
		which will appear in {\sf begin}'s error code.
\end{itemize}

% do they track changes in pthread-style TLS? or?
{\bf Implementations.}
Software TM implementations (STM) typically function as a library,
tracking staged memory accesses in local memory,
and aborting whenever a conflict is detected between two transactions' tracked accesses \cite{stm-pldi06}.
Hardware TM implementations (HTM) use processor-level hardware support,
which stages changes in per-CPU cache lines,
and may abort for STM's reason above \cite{htm-experience},
or additionally whenever a conflict is detected between one transaction's traced access and {\em any} other memory access,
or whenever a conflict occurs on the same cache line, not necessarily the same address,
or in case of any system interrupt or cache overflow.

{\bf Terminology.}
The world of hardware transactional memory is home to several more confusing acronyms in addition to ``HTM''.
Transactional Synchronization Extensions (TSX)
refers to Intel's implementation of HTM on the Haswell microarchitecture
\cite{htm-haswell}.
Restricted Transactional Memory (RTM)
refers to the {\tt xbegin}, {\tt xend}, and {\tt xabort} subset of TSX instructions,
which of course correspond to {\sf begin}, {\tt end}, and {\tt abort} listed above,
as well as {\tt xtest}, an instruction which returns whether or not the CPU is executing transactionally.
GCC and Clang expose these as C/C++ intrinsics named {\tt \_xbegin()}, {\tt \_xend()}, {\tt \_xabort()}, and {\tt \_xtest()}
\cite{htm-gcc}.
Hardware Lock Elision (HLE)
refers to the {\tt xacquire} and {\tt xrelease} subset of TSX instructions,
which extend the traditional interface to offer a slightly higher-level way to access the CPU feature,
optimized for simplicity for locking-like synchronization patterns
% other possible things to cite here:
% https://lwn.net/Articles/534758/
% https://software.intel.com/en-us/node/683688
\cite{hardware-lock-elision}.
In this thesis I focus on RTM, the more general (i.e., expressive (i.e., bug-prone)) interface,
and among all these acronyms restrict myself to ``HTM''
(when referring to transactional memory as a concurrency primitive in the abstract)
and ``TSX''
(when referring to Intel's implementation and/or GCC's intrinsics interface).
The non-pedantic reader may treat these as interchangeable.

The example TSX program from Figure~\ref{fig:htm-example} (\sect{\ref{sec:overview-tm}})
is reproduced here for the reader's convenience.

\begin{figure}[h]
	\input{fig-htm-example}
	\caption{Example TSX program.}
	\label{fig:htm-example-reproduced}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Concurrency Model}
\label{sec:tm-design}

While up to now Landslide's tested programs' concurrency has been limited to timer-driven thread scheduling,
HTM presents a fundamentally new dimension of nondeterminism,
namely the hardware's ability to revert execution sequences
and the delayed visibility of changes to other threads.
In order to efficiently test HTM programs in Landslide,
in this section
I develop a simpler concurrency model and offer a proof of equivalence to HTM execution semantics.
I make two major simplifications:
simulating transaction aborts as immediate failure injections,
and treating transaction atomicity as a global mutex during data-race analysis;
and provide corresponding equivalence proofs.

{\bf Notation.} Let $I = TN_1@L_1, TN_2@L_2, ... TN_n@L_n$,
with $N_i$ a thread ID and $L_i$ a code line number,
denote the execution sequence of a program as it runs according to the specified thread interleaving.
This serialization of concurrent execution is told from the perspective of all CPUs at once
and hence assumes sequential consistency.
% TODO: fix sect refrance
For discussion of relaxed memory models refer to Section~\ref{sec:warpzone}.

\subsection{Example}

Consider again the program in Figure~\ref{fig:htm-example-reproduced}.
Note that the C-style {\tt x++} operations, when compiled into assembly, %\cite{printy},
results in multiple memory accesses which can be interleaved with other threads.
% TODO: fix line numbers
% TODO: put "void count" back in the mario-fixed example program, and fix ALL line numbers thruout proofs
\[
\begin{tabular}{ll}
	$2a$ & \texttt{temp <- x;} \\
	$2b$ & \texttt{temp <- temp + 1;} \\
	$2c$ & \texttt{x <- temp;} \\
\end{tabular}
\]

% T1 T2 T3 but latex doesn't allow #s in cmd names
\newcommand\ti{\ensuremath{\hilight{lavender}{\mathbf{T1}}}\xspace}
\newcommand\tj{\ensuremath{\hilight{seafoam}{\mathbf{T2}}}\xspace}
\newcommand\tk{\ensuremath{\hilight{salmon}{\mathbf{T3}}}\xspace}

\newcommand\tiat[1]{\ensuremath{\hilight{lavender}{\mathbf{T1}@#1}}\xspace}
\newcommand\tjat[1]{\ensuremath{\hilight{seafoam}{\mathbf{T2}@#1}}\xspace}
\newcommand\tkat[1]{\ensuremath{\hilight{salmon}{\mathbf{T3}@#1}}\xspace}

If these instructions from the {\tt x++} in the transaction are preempted,
with another thread's access to {\tt x} interleaved in between,
the transaction will abort.
So, the interleaving
\[
	\tiat{1}, \tiat{2a}, \tiat{2b}, \tjat{1}, \tjat{2}, \tjat{3}, \tiat{2c}, \tiat{3}
\]
or, henceforth abbreviated for clarity:
\[
	\tiat{1-2b}, \tjat{1-3}, \tiat{2c-3}
\]
is not possible; rather, \ti will fall into the backup path:
\[
	\tiat{1-2b}, \tjat{1-3}, \tiat{4-7}
\]
However, the {\tt x++} operation from the failure path (correspondingly $6a$, $6b$, $6c$)
{\em can} be thusly separated with conflicting accesses interleaved in between,
since the mutex only protects the failure path against other failure paths,
but not against the transaction itself.
So (assuming {\tt x} is intended to be a precise counter rather than a sloppy one),
%losing one of the increments to which constitutes a bug),
the following interleaving exposes a bug:%
\footnote{Note also that this bug requires either at least 3 threads or at least 2 iterations between 2 threads to expose;
this highlights MC's dependence on its test cases to produce meaningful state spaces in the first place.}
\[
	\tiat{1-2b}, \tjat{1-3}, \tiat{4-6b}, \tkat{1-3}, \tiat{6c-7}
\]
Prior work \cite{tm-benchmark-cmu} proposed the idiom shown in Figure~\ref{fig:htm-fixed}
to exclude this family of interleavings,
which shows that correctly synchronizing even the simplest transactions may be surprisingly difficult or complex.
%further motivating our research.

\begin{figure}[t]
	\begin{center}
		\begin{tabular}{ll}
		%\texttt{void count() \{} \\
		%\texttt{~~~~for (int i = 0; i < 1000; i++) \{} \\
		& \texttt{\ctype{bool} prevent\_transactions = \const{false};} \\
		\\
		0 & \texttt{\flow{while} (prevent\_transactions) \flow{continue};} \\
		1 & \texttt{\flow{if} ((status = \call{\_xbegin}()) == \const{\_XBEGIN\_STARTED}) \{} \\
		2 & \texttt{~~~~\flow{if} (prevent\_transactions)} \\
		3 & \texttt{~~~~~~~~\call{\_xabort}();} \\
		4 & \texttt{~~~~x++;} \\
		5 & \texttt{~~~~\call{\_xend}();} \\
		6 & \texttt{\} \flow{else} \{} \\
		7 & \texttt{~~~~\call{mutex\_lock}(\&m);} \\
		8 & \texttt{~~~~prevent\_transactions = \const{true};} \\
		9 & \texttt{~~~~x++;} \\
		A & \texttt{~~~~prevent\_transactions = \const{false};} \\
		B & \texttt{~~~~\call{mutex\_unlock}(\&m);} \\
		C & \texttt{\}} \\
		%\texttt{~~~~\}} \\
		%\texttt{\}} \\
		\end{tabular}
	\end{center}
	\caption{Variant of the program in Figure~\ref{fig:htm-example-reproduced},
		with additional synchronization to protect the failure path from the transactional path.
		The optional line 0 serves to prevent a cascade of failure paths
		for the sake of performance
		by allowing threads to wait until transacting is safe again.}
	\label{fig:htm-fixed}
\end{figure}

\subsection{Modeling Transaction Failure}

In the previous section's examples,
the way I stated interleavings such as $\tiat{1-2c}, \tjat{1-3}, \tiat{4-7}$%
\footnote{For a clearer example here I have reordered \ti's write to {\tt x} before \tj's part, compared to before.}
glossed over the HTM execution semantics underlying how such a sequence of operations would be carried out.
For example, thread 1's write during $2c$ is not actually visible to thread 2,
such as it would be under a thread-scheduling-only concurrency model.

Intel's official TSX documentation summarizes its interface and behaviour in prose \cite{intel-tsx-overview};
to the best of my knowledge,
no formal definition exists which could be verified in a mechanized logical framework \cite{x86-semantics}.
Hence the proofs in this section will likewise be in prose.
To summarize HTM's execution semantics:

% TODO FIXME: this is not genral semantics, this is jut as epplies to the examineple. rephrase
\begin{enumerate}
	\item any modifications to shared state (such as $2c$) by \ti are not visible to \tj during its execution,
		despite \tj being executed afterwards, and
	\item all local and global state changes by \ti between lines $1$ and $2c$ are discarded when jumping to line $4$.
\end{enumerate}

% there shouldbe a better word than 'requires' here.. benefits from.. "the point of it is"..
While use of HTM in production requires the performance advantage
of temporarily staging such accesses in local CPU cache,
model checking such programs need be concerned only with the program's {\em observable} behaviours.
I claim that MCing the simpler interleaving $\tiat{1}, \tjat{1-3}, \tiat{4-7}$
is an equivalent verification as MCing the one above;
in fact, this interleaving suffices to check
all observable behaviours
of all interleavings
of all subsets of $\tjat{1-3}$
with all subsets of $\tiat{2a-2c}$,
whether they share a memory conflict or not.
Stated formally, let:

\newcommand\tii{\ensuremath{\hilight{lavender}{\mathbf{Ti}}}\xspace}
\newcommand\tjj{\ensuremath{\hilight{seafoam}{\mathbf{Tj}}}\xspace}
\newcommand\tkk{\ensuremath{\hilight{salmon}{\mathbf{Tk}}}\xspace}

\newcommand\tiiat[1]{\ensuremath{\hilight{lavender}{\mathbf{Ti}@#1}}\xspace}
\newcommand\tjjat[1]{\ensuremath{\hilight{seafoam} {\mathbf{Tj}@#1}}\xspace}
\newcommand\tkkat[1]{\ensuremath{\hilight{salmon}  {\mathbf{Tk}@#1}}\xspace}

\begin{itemize}
	\item $\tiiat{\alpha}$ be an HTM begin operation,
	\item $\tiiat{\beta_1}\dots\tiiat{\beta_n}$ be the transaction body (with $\beta_n$ the HTM end call),
	\item $\tiiat{\phi_1}\dots\tiiat{\phi_m}$ be the failure path, and
	\item $\tiiat{\omega_1}\dots\tiiat{\omega_l}$ be the subsequent code executed unconditionally.
\end{itemize}
Note that arbitrary code may not be structured to distinguish these as nicely as in our examples;
e.g., more code may exist in the success branch after {\tt \_xend()};
such would be considered part of $\omega$ here.

Then, without loss of generality (for any number of other threads \tjj/\tkk, and for any number of thread switches away from \tii during the transaction):
\vspace{1em}

\begin{lemma}[Equivalence of Aborts]
	\label{lem:equiv}
	For any interleaving prefix
	\[
	\begin{tabular}{c}
		$\tiiat{\alpha},\tiiat{\beta_1}\dots\tiiat{\beta_b},$\\
		$\tjjat{\gamma_1}\dots\tjjat{\gamma_j},$ \\
		$\tkkat{\kappa_1}\dots\tkkat{\kappa_k},$ \\
		$\tiiat{\beta_{b+1}}$ % $\dots\tiiat{\beta_{n-1}}$ -- excluded bc might abort
	\end{tabular}
	\]
	with $b<n$, $j \ne i$, $k \ne i$, etc., either:
	\begin{enumerate}
		\item $\tiiat{\alpha},\tjjat{\gamma_1}\dots\tjjat{\gamma_j},\tkkat{\kappa_1}\dots\tkkat{\kappa_k},\tiiat{\phi_1}\dots$
			(conflicting case), or
		\item $\tiiat{\alpha},\tiiat{\beta_1}\dots\tiiat{\beta_b}\dots\tiiat{\beta_n},\tjjat{\gamma_1}\dots\tjjat{\gamma_j},\tkkat{\kappa_1}\dots\tkkat{\kappa_k}$
			(independent case)
	\end{enumerate}
	exists and is observationally equivalent.
\end{lemma}

\begin{proof}
	Case on whether the operations by \tjj and/or \tkk have any memory conflicts (read/\allowbreak{}write or write/write)
	with $\tiiat{\beta_1}\dots\tiiat{\beta_n}$.
	If so, then the hardware will abort \tii's transaction, discarding the effects of $\tiiat{\beta_1}\dots\tiiat{\beta_n}$
	and jumping to $\tiiat{\phi_1}$,
	satisfying case 1.
	% TODO: give a secrion referenace
	Otherwise, by DPOR's definition of transition dependence \cite{dpor}, %,landslide-phdthesis},
	$\tiiat{\beta_{b+1}}\dots\tiiat{\beta_n}$ is independent with the transitions of \tjj and \tkk,
	may be successfully executed until transaction commit,
	and reordering them produces an equivalent interleaving,
	satisfying case 2.
\end{proof}

The claim's second part follows naturally.
\vspace{1em}

\begin{theorem}[Atomicity of Transactions]
	\label{thm:atom}
	For any state space $S$ of a transactionally-concurrent program,
	an equivalent state space exists in which all transactions are either executed atomically or aborted immediately.
\end{theorem}

\begin{proof}
	For every $I \in S$ with $\tiiat{\alpha},\tiiat{\beta_1}\dots\tiiat{\beta_b},$ $\tjjat{\dots},\tkkat{\dots},\tiiat{\beta_{b+1}} \in I$,
	apply Lemma~\ref{lem:equiv} to obtain an equivalent interleaving $I'$ satisfying the theorem condition.
	The resulting $S'$ can then be MCed without ever simulating HTM rollbacks.
\end{proof}

\subsection{Memory Access Analysis}

Next comes the issue of memory accesses within transactions with regard to data-race analysis (\sect{\ref{sec:background-datarace}}).
Theorem~\ref{thm:atom} provides that the body of all transactions may be executed atomically within the MC environment.
While they may interleave between other non-transactional sequences,
no other operations (whether transactional or not) will interrupt them.
I claim this level of atomicity is equivalent to that provided by a global lock,
and hence abstracting it as such in Landslide's data-race analysis is sound.

Let $\tiiat{\mu},\tjjat{\nu}$ be a pair of memory accesses to the same address, at least one a write,
in some transactional execution $I$ normalized under Lemma~\ref{lem:equiv}.
Then let $\mathsf{lockify}_m(\tkkat{L})$ denote a function over instructions in $I$,
which replaces $\tkkat{L}$ with $\tkkat{\mathsf{lock}(m)}$ if $L$ is a successful HTM begin,
with a no-op if $L$ is a transaction abort,
or with $\tkkat{\mathsf{unlock}(m)}$ if $L$ is an HTM end,
or no replacement otherwise.
Finally, let $I' = \exists m. \mathsf{lockify}_m(I)$,
the execution with the boundaries of all successful transactions replaced by an abstract global lock.
Lemma~\ref{lem:equiv} guarantees mutual exclusion of $m$.
\vspace{1em}

\begin{theorem}[Transactions are a Global Lock]
	$\tiiat{\mu},\tjjat{\nu}$ is a data race in $I$ iff it is a data race in $I'$.
\end{theorem}

\begin{proof}
I prove one case for each variant definiton for data races supported in Landslide \cite{quicksand}.
For each, I state below what it means to race in an execution with synchronizing HTM instructions.

\begin{itemize}
	\item {\bf Limited Happens-Before.}
		%Under this definition,
		To race in $I$ they must be reorderable at instruction granularity,
		at least one with a thread switch immediately before or after.
		\cite{tsan,hybriddatarace}.
		\begin{itemize}
			\llitem $I \Rightarrow I'$:
				If $\tiiat{\mu},\tjjat{\nu}$ race in $I$,
				then they cannot both be in successful transactions,
				or else placing \tiiat{\mu} within the boundaries of \tjjat{\nu}'s transaction
				would cause the latter to abort, invalidating \tjjat{\nu}, or vice versa.
				Hence they will not both hold $m$ in $I'$.
				Otherwise their lock-sets and DPOR dependence relation remain unchanged.
			\llitem $I' \Rightarrow I$:
				If $\tiiat{\mu},\tjjat{\nu}$ race in $I'$,
				both corresponding threads cannot hold $m$;
				WLOG let $\tii$ not hold $m$ during $\tiiat{\mu}$.
				Then in $I$, $\tiiat{\mu}$ is not in a transaction.
				With the remainder of their lock-sets still disjoint,
				and still not DPOR-dependent, $\tjjat{\nu}$ (or its containing transaction)
				can then be reordered directly before or after $\tiiat{\mu}$.
		\end{itemize}
	\item {\bf Pure Happens-Before.}
		WLOG fix $\tiiat{\mu} \prec \tjjat{\nu} \in I$.
		Then to race in $I$ there must be no pair of synchronizing instructions
		$\tiiat{\epsilon}$ (a release edge) and $\tjjat{\chi}$ (an acquire edge) such that
		\[
			\tiiat{\mu} \prec \tiiat{\epsilon} \prec \tjjat{\chi} \prec \tjjat{\nu} \in I
		\]
		to update the vector clock epoch between \tiiat{\mu} and \tjjat{\nu} \cite{djit,fasttrack}.
		\begin{itemize}
			\llitem $I \Rightarrow I'$:
				If $\tiiat{\mu},\tjjat{\nu}$ race in $I$,
				then they cannot both be in successful transactions,
				or else Lemma~\ref{lem:equiv} normalization would provide
				the corresponding HTM end and begin for $\tiiat{\epsilon}$ and $\tjjat{\chi}$ respectively.
				Hence there will be no unlock/lock pair on $m$ in $I'$ to satisfy the above sequence.
			\llitem $I' \Rightarrow I$:
				If $\tiiat{\mu},\tjjat{\nu}$ race in $I'$,
				then they cannot both hold $m$,
				or else $\mathsf{lockify}_m$ would provide the corresponding
				unlock and lock for $\tiiat{\epsilon}$ and $\tjjat{\chi}$ respectively.
				Hence there will be no HTM end/begin pair in $I$ to satisfy the above sequence.
		\end{itemize}
		%In both cases $I$ and $I'$ otherwise have the same synchronizing instructions.
\end{itemize}
Hence, data-race analysis is sound when transaction boundaries are replaced by an abstract global lock.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Implementation}

Support for TSX programs in Landslide is implemented in three parts, broadly speaking,
the user interface plus one internal part corresponding to each proof above.

\subsection{User Interface}

Landslide provides its own ``implementation'' of the TSX interface,
which matches GCC's interface exactly,
located in {\tt 410user/inc/htm.h}
under {\tt pebsim/p2-basecode/}.
The interface is implemented in {\tt 410user/libhtm/htm.c},
perhaps surprisingly, as totally empty functions:
Landslide hooks these functions' addresses during instrumentation
and inserts preemption points, failure injections,
et cetera as necessary whenever the execution encounters them.

Transactional test programs should be ported to the Pebbles userspace if not already,
then any use of compiler HTM intrinsics replaced with Landslide's interface%
\footnote{Attempts to execute a real TSX instruction under Landslide, instead of using the custom interface,
will be reported as ``invalid opcode'' bugs,
as neither of its supported simulation platforms support the feature.
}.
They should then be put in either {\tt 410user/progs/} or {\tt user/progs/}
and the {\tt 410TESTS} or {\tt STUDENTTESTS} line (respectively) of {\tt config-incomplete.mk} be edited to add the test name,
before running {\tt p2-setup.sh} (\sect{\ref{sec:landslide-setup}})
% TODO(implementation): Provide an obfuscated refp2?
on a (hopefully) correct P2 as usual.
Several example HTM tests are provided as {\tt 410user/progs/htm*.c}.

Finally, Quicksand supports the following command-line options to enable various sets of HTM features within Landslide.
Each option requires each of the previously-listed options to also be present.
\begin{itemize}
	\item {\tt -X} (for ``tsX'' or ``Xbegin'') enables the basic features:
		preemption points on each {\tt \_xbegin()} and {\tt \_xend()} call,
		failure injection on each of the former
		(always returning {\tt \_XABORT\_RETRY} as the failure code)
		(\sect{\ref{sec:txn-failure}}),
		and treatment of transactional regions during data-race analysis (\sect{\ref{sec:txn-datarace}}).
	\item {\tt -A} (for ``xAbort codes'')
		enables multiple xabort failure codes (\sect{\ref{sec:txn-failure}}).
	\item {\tt -S} (for ``Stm'')
		disables the {\tt \_XABORT\_RETRY} failure reason,
		which causes Landslide to match STM's execution semantics rather than HTM.
		(This feature was used in \cite{sigbovik-htm}~\sect{5}.)
\end{itemize}

\subsection{Failure Injection}
\label{sec:txn-failure}

Each preemption point ({\tt struct hax} as defined in {\tt tree.h})
includes three new fields.
The boolean {\tt h->xbegin} is set if the preemption point occurred at an {\tt \_xbegin()} call.
Then if set (as the tag of an option type),
the twin lists of integers
{\tt h->xabort\_codes\_ever}
and
{\tt h->xabort\_codes\_todo}
store possible error codes this {\tt \_xbegin()} call should be tested to possibly return.
The former list stores all such error codes, whether already tested or yet to be tested,
while the latter serves as a workqueue that indicates only those not already tested yet
(serving an analogous purpose as the {\tt all\_explored} flag for thread scheduling).

\subsubsection{Adding possible abort codes}

Up to three abort codes are considered depending on the testing options listed in the previous section.
All abort codes are simultaneously added to both lists,
unless already present on {\tt xabort\_codes\_ever},
in which case not added to {\tt xabort\_codes\_todo} to avoid duplicate work.
\begin{itemize}
	\item {\tt \_XABORT\_RETRY}:
		When a {\tt xbegin} preemption point is created ({\tt save\_setjmp()}),
		if {\tt -S} is not set,
		both {\tt xabort\_codes} lists are initialized to contain this code.
		This represents the possibility for a hardware transaction to fail
		for reasons outside of the programmer's control such as system interrupts or cache eviction.
		If {\tt -S} is set they are initialized to empty,
		representing either STM's policy of failing only when a true memory conflict arises,
		or a TSX user wrapping all their {\tt \_xbegin()}s in a retry loop such as in \cite{sigbovik-htm}.
	\item {\tt \_XABORT\_CONFLICT}:
		Whenever DPOR detects a memory conflict between two preemption points ({\tt shimsham\_shm()}),
		if {\tt -A} is set,
		for each of the two which are within a transaction,
		it finds the nearest ancestor {\tt xbegin} preemption point by the same thread
		and adds this code to both lists.
	\item {\tt \_XABORT\_EXPLICIT}:
		Whenever the program invokes {\tt \_xabort()} ({\tt sched\_update\_user\_\allowbreak{}state\_machine()}),
		if {\tt -A} is set,
		this code is added to both lists, bitwise-ored with the user-supplied argument code
		as specified in \cite{htm-gcc}.
\end{itemize}

Landslide does not yet check for false sharing,
i.e. read/write or write/write access pairs to different memory addresses that share a cache line.
On real hardware, these would produce {\tt \_XABORT\_CONFLICT} failures,
but to find such access pairs would require extending {\tt mem\_shm\_intersect()}'s set intersection algorithm
to consider a certain degree of ($N$-byte-aligned) fuzziness when comparing addresses,
as well as adding a command-line option to configure said $N$, the cache-line size to simulate.
For now, Landslide (wrongly) lumps false sharing in among the ``spurious'' {\tt \_XABORT\_RETRY} failure reasons.

On HTM, even if the user wraps all such spurious failure reasons in a retry loop,
false sharing should still produce a non-retryable abort,
begetting a discrepancy with STM,
which aborts only when the memory addresses match exactly.
Accordingly, the {\tt -S} option described above provides STM semantics as currently implemented.
Future work could extend Landslide with an option to configure false sharing conflicts
to remain faithful to HTM semantics and still abort even under {\tt -S}.

\subsubsection{Injecting abort codes}
When traversing the state space (\sect{\ref{sec:landslide-statespace}}),
in addition to performing DPOR to select non-independent thread interleavings (\sect{\ref{sec:landslide-dpor}}),
the abort codes under each {\tt xbegin} preemption point are also considered ``marked'' paths which must be tested.
Hence {\tt explore()}, by way of {\tt any\_tagged\_child()},
will pop off the {\tt xabort\_codes\_todo} queue
when it's time to explore that preemption point in the usual depth-first manner%
\footnote{The search order prioritizes abort codes before scheduling other threads
at such preemption points,
which is just an implementation detail, not theoretically necessary}.
The optional abort code is then passed through {\tt arbiter\_append\_choice()}/{\tt arbiter\_pop\_choice()}
to {\tt cause\_transaction\_failure()},
which edits the simulation state ({\tt \%eip} and {\tt \%eax})
to force {\tt \_xbegin()} to return the provided code.

\subsubsection{Tracking abort sets}
\label{sec:tm-abortsets}

% In contrast to Landslide's implementation of the sleep set reduction (\sect{\ref{sec:landslide-explore}}),
% blah blah which is implicit and works in retrospect by pruning at tag-sibling time, which figures out what would have been in a sleep set by analyzing the past independences and explored siblings,
% this cannot work that way,
% because in the first place, the former works with the "keep running current thread" ordering heuristic,
% (which automatically takes care of the first half of the sleep set problem)
% whereas no such similar heuristic exists (can exist??) for xbegin PPs, we always just run the success path first

% TODO

% ok so heres an example of why this is rly hard

% ==== thread 1 ====
% if (xbegin()) {
% 	xabort(0) // whatever, executed in a past branch to be aborting in the present
% } else {
% 	x++;
% }
%
% ==== thread 3 ====
% conflict_with_thread_2 = true;
%
% ==== thread 2 ====
% if (xbegin()) {
% 	conflict_with_thread_2 ++ ; // whatever, just conflict and fail w thr 3
% } else {
% 	x++; // conf with thr 1
% }
%
% so we execute this interleaving (T1 abort, T3, T2 abort)
% dpor sees the conflict between T2 and T1 and wants to tag T2 to run as transition(T1-abort)'s sibling to run
% it would use the "abort set" of (T2 [CONFLICT], T1 [EXPLICIT])
% but like, how do we know that thread 3 is necessary to reach this?
% and even, how do we know that it's valid to inject CONFLCIT in T2?
% you pretty much hafta reorder T3 (dpor doesn't see why you need to)
% but if you inject conflict in T2 without also making sure T3 executes "near" it, that might not necessarily be sound

\subsection{Data Race Analysis}
\label{sec:txn-datarace}

When a thread returns {\tt \_XBEGIN\_STARTED} from {\tt \_xbegin()}
(analogous to {\tt mutex\_trylock()}),
Landslide's scheduler sets the {\tt user\_txn} action flag for that thread (\sect{\ref{sec:landslide-scheduler}}),
and if using Pure Happens-Before,
applies \textsc{FT acquire} (\sect{\ref{sec:landslide-phb}}) using a dummy lock address to represent the abstract global lock.
When a thread reaches {\tt \_xend()},
the flag is cleared,
and under Pure Happens-Before,
\textsc{FT release} is applied.
Then when {\tt check\_locksets()} compares an access pair,
under Limited Happens-Before,
it is considered a data race only if at least one thread's {\tt user\_txn} was not set
in addition to the usual conditions; % in multiplication to..? && c.c
under Pure Happens-Before,
the vector clocks are simply checked as usual.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Evaluation}

While prior work has focused on verifying implementations of transactional memory themselves
\cite{specifying-verifying-tm,tm-correctness,tm-completeness,mc-tm-with-spin},
Landslide is to the best of my knowledge the first model checker to support transactional client code.
% mumble smth new concurrency model
Accordingly, there is no baseline against which to compare Landslide's performance.
Likewise, since Landslide's HTM semantics emulation relies on the equivalences proved in \sect{\ref{sec:tm-design}},
I did not actually implement an HTM-style speculative-execution-and-rollback simulation mode.
% so have no basis to measure how much it saves, or smth like that
On this count, at least,
I hope the reader finds it self-evident that
the equivalence proofs provide exponential state space reduction
compared to actually testing (and thereafter aborting) every combination of instructions within transactions.
Beyond those, this chapter's evaluation will take a relatively green-field and exploratory approach.
I pose the following evaluation questions.

% on one hand,
% comparing with \cite{mc-tm-with-spin}, succeeding for (2,3) then burning out with (2,4) is consistent with prior work
% on the other hand, they were verifying STM implementation, which is no doubt more operations per iteration?
% whatever, don't need to make this argument

\begin{enumerate}
	\item How quickly does Landslide find bugs in incorrect transactional programs? %of varying sizes?
	\item Does Landslide find any previously-unknown bugs in real-world transactional code?%
		\footnote{The savvy reader will realize that whether or not the author poses this evaluation question to begin with spoils its answer.}
	%\item How quickly does Landslide verify correct transactional programs of varying sizes?
	\item How does Iterative Deepening's (\sect{\ref{sec:quicksand-id}})
		performance compare to Maximal State Space mode (\sect{\ref{sec:landslide-quicksand-options}})?%
		\footnote{Not necessarily related to HTM,
		but the latter was implemented well after Chapter~\ref{chap:quicksand}'s conference paper was published,
		so this was the most convenient test suite to evaluate it on.}
	\item How well does Landslide's verification scale
		with increasing thread/iteration count for correct transactional programs?
	\item What further reduction can be achieved beyond that provided by
		the global-lock/failure-injection equivalences?
	%\item By the way, should MC research papers quantify variance in their CPU-time performance experiments?
\end{enumerate}

\subsubsection{Experimental setup}

The evaluation suite comprises several hand-written unit tests,
microbenchmarks and transactional data structures from \cite{tm-benchmark-cmu},
a transactional spinlock from \cite{spinlock-rtm-github},
and various combinations thereof,
as follows.

% notes to self abour marios tests
% counter: tests k threads on n iterations, benchmark xchg-aways vs txn. removed benchmarking, added assertion that no counts are lost.

\begin{itemize}
	\item Microbenchmarks
	\begin{itemize}
		\item {\tt htm1}: The bug from Figure~\ref{fig:htm-example}. %(assertion failure on {\tt count}).
		\item {\tt htm2}: The fixed version as in Figure~\ref{fig:htm-fixed}.
		\item {\tt counter}: Microbenchmark version of {\tt htm2} which replaces the complex locking failure path with an atomic {\tt xadd}, from \cite{tm-benchmark-cmu}.
		\item {\tt swap}: Microbenchmark that swaps values in an array, from \cite{tm-benchmark-cmu}.
		\item {\tt swapbug}: {\tt swap} modified to introduce circular locking in the failure path. %(deadlock).
	\end{itemize}
	\item Data structure tests
	\begin{itemize}
		\item {\tt avl\_insert}: AVL tree concurrent insertion test \cite{tm-benchmark-cmu}.
		\item {\tt avl\_fixed}: {\tt avl\_insert} with the AVL bug fixed (spoilers!!).
		\item {\tt map\_basic}: Separate-chaining hashmap concurrent insertion test \cite{tm-benchmark-cmu}.
		\item {\tt map\_basicer}: {\tt map\_basic} modified with a larger initial size to skip the resizing step.
		\item {\tt avl\_mutex}: {\tt avl\_fixed} with transactional sections simplified by abstraction into a mutex.
		\item {\tt map\_mutex}: {\tt map\_basic} simplified likewise.
	\end{itemize}
	\item Lock abstraction tests
	\begin{itemize}
		\item {\tt lock}(): Checks that multiple threads using a transactional lock
			cannot access the critical section simultaneously.
			% htm_spinlock and htm_mutex
		\item {\tt lock\_fast}(): Checks that a transactional lock's fast path will not suffer conflict aborts
			if its client threads' critical sections are independent.
			% htm_spinlock_fastpath and htm_mutex_fastpath
	\end{itemize}
		These are each parameterized over implementations {\tt spinlock} (from \cite{spinlock-rtm-github}),
		{\tt spin\_\allowbreak{}fixed} (spoilers!!),
		and {\tt mutex} (replaces the spinlock with a Landslide-annotated P2 mutex to reduce state space size).
\end{itemize}

The notation {\tt testname}$(K,N)$ will denote a test configuration of $K$ threads, each running $N$ iterations of the test logic.
All tests were run on an 8-core 2.7GHz Core i7 with 32 GB RAM.
Reported CPU-times include time spent on all state spaces
Quicksand saw fit to run,
not just the maximal or the buggy state space;
for verification tests (run with {\tt -M}),
this still includes abandoned smaller jobs that were run to saturate the set of data-race preemption points
(Chapter~\ref{chap:quicksand}).
To minimize variance in CPU-time measurements,
I ensured the test machine was not loaded beyond normal web browser use,
and ran only one instance of Landslide at a time
\cite{sigbovik-htm}.

Finally, the keen-eyed reader will notice the state space sizes reported here
differ from those reported in \cite{sigbovik-htm}.
All experiments have been re-run
on account of three updates to Landslide's exploration algorithm
implemented since then:
the sleep sets optimization for DPOR (\sect{\ref{sec:landslide-explore}}, commit 0447666),
the {\tt thrlib\_function} directive
to mark internal thread library logic as trusted (\sect{\ref{sec:landslide-config-landslide}}, commit 64a02e4),
and fixing a soundness bug in which Landslide could neglect to inject transaction failures
immediately after a thread switch (commit dcae85b).
On account of the former two updates, some state spaces may be smaller than before;
on account of the third, some may be larger.
These updates do not discredit the bug-finding results (a bug is a bug),
but the previously-published verification results should be considered outdated.

\newcommand\ETA[1]{\hilight{brownish}{{\em #1}}\xspace}
\newcommand\cpu[1]{\hilight{darkcyan}{{#1}}\xspace}
\newcommand\wtm[1]{\hilight{lime}{{#1}}\xspace}
\newcommand\ints[1]{\hilight{pinkish}{{#1}}\xspace}
\newcommand\ETAdag[1]{\ETA{\ensuremath{\dagger}#1}}

\subsection{Bugs}
\label{sec:tm-eval-bugs}

\begin{table*}[p]
	\begin{center}
		\footnotesize
	\begin{tabular}{cc||r|r|r||r|r|r|r}
			&	&\multicolumn{3}{c||}{\bf Quicksand mode}&\multicolumn{4}{c}{{\bf Maximal state space mode} ({\tt -M})} \\
		\bf buggy test	& \bf params&\cpu{\bf cpu (s)}&\wtm{\bf wall (s)}&\ints{\bf int's}&\cpu{\bf cpu (s)}&\wtm{\bf wall (s)}&\ints{\bf int's}& \ETA{\bf SS size (est.)} \\
		\hline
		\hline
		{\tt htm1}
			& 2,1	&\cpu{  45.78}&\wtm{9.70}&\ints{21}& \cpu{*8.21}& \wtm{*5.78}& \ints{5	}& \ETA{12} \\
			% 5 interleavings tested; 1 preemptions; job time 2s; pldi time 5781000; new-fixed pldi cputime 8214902
		(assertion)
			& 2,2	&\cpu{  84.14}&\wtm{13.59}&\ints{*33}& \cpu{*8.42}& \wtm{*5.97}& \ints{9	}& \ETA{102} \\
			% 9 interleavings tested; 1 preemptions; job time 2s; pldi time 5972944; new-fixed pldi cputime 8418505
			& 2,3	&\cpu{ 131.91}&\wtm{20.44}&\ints{*73}& \cpu{*8.80}& \wtm{*6.31}& \ints{17	}& \ETA{819} \\
			% 17 interleavings tested; 1 preemptions; job time 2s; pldi time 6314386; new-fixed pldi cputime 8795639
			& 2,4	&\cpu{ 255.75}&\wtm{37.56}&\ints{257}& \cpu{*9.72}& \wtm{*7.19}& \ints{33	}& \ETA{6553} \\
			% 33 interleavings tested; 1 preemptions; job time 3s; pldi time 7193093; new-fixed pldi cputime 9717902
		\cline{2-9}
			& 3,1	&\cpu{ 114.06}&\wtm{17.45}&\ints{*15}& \cpu{*8.15}& \wtm{*5.71}& \ints{5	}& \ETA{76} \\
			% 5 interleavings tested; 1 preemptions; job time 2s; pldi time 5710637; new-fixed pldi cputime 8153919
			& 3,2	&\cpu{ 109.60}&\wtm{26.16}&\ints{49}& \cpu{*8.37}& \wtm{*5.91}& \ints{9	}& \ETA{3686} \\
			% 9 interleavings tested; 1 preemptions; job time 2s; pldi time 5907009; new-fixed pldi cputime 8367804
			& 3,3	&\cpu{ 124.80}&\wtm{20.40}&\ints{*73}& \cpu{*8.71}& \wtm{*6.26}& \ints{17	}& \ETA{176947} \\
			% 17 interleavings tested; 1 preemptions; job time 2s; pldi time 6263778; new-fixed pldi cputime 8711332
			& 3,4	&\cpu{ 227.49}&\wtm{35.15}&\ints{*161}& \cpu{*10.36}& \wtm{*7.15}& \ints{33	}& \ETA{8493465} \\
			% 33 interleavings tested; 1 preemptions; job time 3s; pldi time 7153968; new-fixed pldi cputime 10355961
		\cline{2-9}
			& 4,1	&\cpu{ 53.08}&\wtm{9.79}&\ints{*15}& \cpu{*7.77}& \wtm{*5.30}& \ints{5	}& \ETA{460} \\
			% 5 interleavings tested; 1 preemptions; job time 2s; pldi time 5295360; new-fixed pldi cputime 7770389
			& 4,2	&\cpu{117.07}&\wtm{19.09}&\ints{*33}& \cpu{*8.32}& \wtm{*5.59}& \ints{9	}& \ETA{132710} \\
			% 132710 9 interleavings tested; 1 preemptions; job time 2s; pldi time 5588002; new-fixed pldi cputime 8317173
		\hline
		{\tt swapbug}
			& 2,1	&\cpu{ 70.95}&\wtm{13.45}&\ints{*16}& \cpu{*28.32}& \wtm{*11.29}& \ints{33	}& \ETA{73} \\
			% 33 interleavings tested; 1 preemptions; job time 3s; pldi time 11286074; new-fixed pldi cputime 28324164
		(deadlock)
			& 2,2	&\cpu{ 107.28}&\wtm{*17.45}&\ints{*146}& \cpu{*30.55}& \wtm{13.50}& \ints{85	}& \ETA{860} \\
			% 85 interleavings tested; 1 preemptions; job time 5s; pldi time 13499606; new-fixed pldi cputime 30550037
			& 2,3	&\cpu{ 280.05}&\wtm{38.70}&\ints{*352}& \cpu{*32.27}& \wtm{*19.42}& \ints{217	}& \ETA{9120} \\
			% 217 interleavings tested; 1 preemptions; job time 11s; pldi time 19419243; new-fixed pldi cputime 37272562
			& 2,4	&\cpu{ 617.94}&\wtm{*81.50}&\ints{*834}& \cpu{*53.44}& \wtm{35.80}& \ints{537	}& \ETA{91239} \\
			% 537 interleavings tested; 1 preemptions; job time 27s; pldi time 35796053; new-fixed pldi cputime 53441873
		\cline{2-9}
			& 3,1	&\cpu{*1275.04}&\wtm{*163.42}&\ints{*771}&\cpu{*65.13}& \wtm{52.23}& \ints{980   }& \ETA{3417} \\
			% 980 interleavings tested; 2 preemptions; job time 40s; pldi time 52225013; new-fixed pldi cputime 65125824
			& 3,2	&\ETA{--}&\ETA{>30m}&\ETA{--}&\ETA{--}&\ETA{>1h}&\ETA{--}& \ETA{2256553} \\
			% [JOB 8] progress: 63137/2256553 brs (2.797940%), ETA 130d 19h 28m 29s (elapsed 59m 45s)
		\hline
		{\tt avl\_insert}
			& 2,2	&\cpu{488.07}&\wtm{64.77}&\ints{*83}	&\cpu{*66.32}&\wtm{*31.81}&\ints{127}&\ETA{254078} \\
		(segfault+)
			& 2,3	&\cpu{2670.87}&\wtm{*330.45}&\ints{*3066}&\cpu{*256.94}&\wtm{193.71}&\ints{1391}&\ETA{22773586} \\
			& 2,4	&\cpu{*3259.37}&\wtm{*436.50}&\ints{*1639}&\ETA{--}&\ETA{>1h}&\ETA{--}& \ETA{91704203} \\
			% [JOB 12] progress: 28140/91704203 brs (0.030686%), ETA 18y 54d 18h 42m 58s (elapsed 51m 12s)
		\cline{2-9}
			& 3,1	&\cpu{222.02}&\wtm{40.04}&\ints{*28}	&\cpu{*64.70}&\wtm{*25.59}&\ints{53}&\ETA{343250} \\
			% 53 interleavings tested; 3 preemptions; job time 6s; pldi time 25585758; new-fixed pldi cputime 64695226
			& 3,2	&\cpu{*1569.09}&\wtm{*216.85}&\ints{*209}&\ETA{--}&\ETA{>1h}&\ETA{--}& \ETA{466977272} \\
			%[JOB 38] progress: 3812/466977272 brs (0.000816%), ETA INF (elapsed 13m 50s)
		\hline
			\multicolumn{5}{c}{\em every quicksand above this line needs rerun} \\ \hline
		{\tt lock\_fast}
		%({\tt spinlock})
			& 2,1	&\cpu{10.48}&\wtm{3.22}&\ints{2}&\cpu{*2.98}&\wtm{*2.98}&\ints{2}& \ETA{8} \\
		(perf)
			& 9,9	&\cpu{37.41}&\wtm{9.96}&\ints{2}&\cpu{*4.93}&\wtm{*4.93}&\ints{2}& \ETA{inf} \\
	\end{tabular}
	\end{center}
	\caption{Landslide's bug-finding performance on various test configurations.
		Iterative Deepening (\sect{\ref{sec:quicksand-id}}), optimized for fast bug-finding,
		is compared against Maximal State Space mode (\sect{\ref{sec:landslide-quicksand-options}}),
		optimized for fast verification.
		For each, I list the CPU-time and wall-clock time elapsed,
		plus the number of interleavings of the ultimately buggy state space tested,
		before the bug was found.
		* marks the winning measurements between each series.
		Lastly, state space estimation (\sect{\ref{sec:landslide-estimate}}),
		though approximate at best,
		confers a sense of the exponential explosion.
		{\bf THESIS DRAFT NOTE:} These numbers all need to be re-run on account of some state space reductions and/or soundness fixes that have been added since these numbers were calculated in \cite{sigbovik-htm}.
	}
	\label{tab:buges}
\end{table*}

Table~\ref{tab:buges}
presents the bug-finding results.
I configured Landslide to run Quicksand's Iterative Deepening algorithm, %(\sect{\ref{sec:quicksand-id}}),
% TODO how many cores?
shown left,
as well as to prioritize the maximal state space, %(\sect{\ref{sec:landslide-quicksand-options}}),
shown right,
% TODO: make 1 hour
each with a time limit of 30 minutes.
Tests {\tt htm1}, {\tt swapbug}, and {\tt avl\_insert} were run with {\tt landslide -X}
(i.e., retry aborts enabled and different abort codes not distinguished);
{\tt lock\_fast} was run with {\tt landslide -X -A -S}
(i.e., suppressing retry aborts, due to the spinlock's use of a retry loop confirmed with manual inspection).

\subsubsection{Finding bugs quickly}

As the test parameters increase,
the multiplicative factor in bug-finding speed (2-4x, eyeballing) is generally smaller
than that of the total number of interleavings (10-100x).
In other words,
should they exist,
Landslide is likely to find bugs reasonably quickly in these transactional programs
despite prohibitive exponential explosion in total state space size.
This corroborates the results from Chapter~\ref{chap:quicksand},
extending its good news to the world of HTM.

\subsubsection{Finding new bugs}

In addition to the bugs I intentionally wrote in {\tt htm1} and {\tt swapbug},
%to my pleasant surprise % only the former of these is a surprise now
Landslide also found two bugs in the ``real-world'' transactional algorithms I tested.
\begin{itemize}
	\item
	{\bf Atomicity violation.}
	{\tt avl\_insert} with any parameters higher than (2,1)
	exposed a previously-unknown bug in the transactional AVL tree.
	Figure~\ref{fig:avlbug} shows the root cause, essentially the {\tt htm1} bug in disguise.
	% landslide-trace-1521093835.18927.html
	This manifested
	alternately
	as a segfault (for test parameters (2,2) and (3,1)) % TODO double check
	% landslide-trace-1521092404.23772.html
	and
	as a consistency-check assertion failure (for test parameters (2,3)).
	The presence of {\tt while (\_retry);} makes the necessary preemption window extremely small
	(between it and {\tt \_xbegin()}),
	making the bug extremely unlikely to manifest under stress testing,
	but Landslide is blind to such matters of chance.

	As a matter of full disclosure,
	while editing this test to make it Landslide-friendly,
	I noted that the loop does not affect the test's possible behaviours,
	only its {\em likely} ones,
	and so removed it to keep the state space size manageable.
	One could then argue that manual inspection is at least partly to credit for this result.
	% TODO justify this
	However, adding the loop back in to dispel any doubt,
	% converting it to a {\tt yield()} loop % TODO: necessary?
	Landslide still finds the bug,
	albeit slightly slower,
	in % TODO how much time
	\item
	{\bf Spurious spinlock abort.}
	{\tt lock\_fast} discovered a spurious transactional-path write conflict
	in the {\tt spinlock} HTM-lock implementation.%
	\footnote{{\tt lock\_fast}'s unusual (9,9) parameter shows that this state space size is constant: %in $(K,N)$:
	DPOR will always either deem all thread transitions independent and end exploration immediately,
	or the test's assertion will trip as soon as the first conflict is found.}
	This ``performance bug'' causes the lock to suffer slow-path spin-locking
	even in cases where the user's thread transitions are completely independent
	(for example, locking the root of an AVL tree,
	then traversing in different directions to make disjoint modifications),
	which the test case detects with {\tt \_xtest()}.
	Figure~\ref{fig:spinlockbug} shows the root cause:
	the {\tt isfree()} routine (corresponding to the AVL's {\tt \_retry})
	used an atomic compare-and-swap that would always write to memory even without modifying it.
	I corrected this in the {\tt spin\_fixed} implementation by replacing it
	with a normal read
	(being used only in the transactional path, no barriers are required to protect it).
	A cursory search on Github found one user of this code,
	a transactional LevelDB implementation \cite{htm-leveldb-github},
	whose author had also noticed and corrected this problem in the same way.

	As another matter of full disclosure,
	I noticed this bug through manual inspection,
	while adapting the spinlock's client code to be Landslide-friendly,
	then wrote {\tt lock\_fast} specifically to target this behaviour,
	so unlike {\tt avl\_insert}, it does not count as Landslide finding a previously-unknown bug.
	However, I feel in retrospect that how and when
	an HTM-backed concurrency abstraction
	will fall into its slow path
	is a reasonable performance property for a user to want to verify,
	so I consider Landslide confirming the bug (and later verifying its absence, in \sect{\ref{sec:tm-verif}})
	a positive result anyway.
\end{itemize}

\begin{figure}[t]
	\begin{center}
		\begin{tabular}{l}
		\texttt{\flow{while} (\_retry);} \\
		\texttt{\flow{if} (\call{\_xbegin}() == \const{SUCCESS}) \{} \\
		\texttt{~~~~\hilight{brickred}{tie}(\_root,inserted) = \call{\_insert}(\_root,n); } \\
		\texttt{~~~~\call{\_xend}();} \\
		\texttt{\} \flow{else} \{} \\
		\texttt{~~~~\call{pthread\_mutex\_lock}(\&\_tree\_lock);} \\
		\texttt{~~~~\_retry = \const{true};} \\
		\texttt{~~~~\hilight{brickred}{tie}(\_root,inserted) = \call{\_insert}(\_root,n); } \\
		\texttt{~~~~\_retry = \const{false};} \\
		\texttt{~~~~\call{pthread\_mutex\_unlock}(\&\_tree\_lock);} \\
		\texttt{\}} \\
		\end{tabular}
	\end{center}
	%\caption{mario man had a very large house. or a castle.}
	\caption{Unmodified code from {\tt htmavl.hpp} showing the previously-unknown bug
		Landslide found in {\tt avl\_insert}.
		The transaction path fails to check {\tt \_retry},
		leading to data races and corruption just as in {\tt htm1}.
		}
	\label{fig:avlbug}
\end{figure}

\begin{figure}[t]
	\begin{center}
		\begin{tabular}{l}
		\texttt{\ctype{bool} \call{hle\_spinlock\_isfree}(\ctype{spinlock\_t} *lock) \{} \\
		\texttt{~~~~\ccomment{// XXX: should be "return lock->v == 0;"}} \\
		\texttt{~~~~\flow{return} \call{\_\_sync\_bool\_compare\_and\_swap}(\&lock->v, \const{0}, \const{0});} \\
		\texttt{\}} \\
		\texttt{\ctype{void} \call{rtm\_spinlock\_acquire}(\ctype{spinlock\_t} *lock) \{} \\
		\texttt{~~~~\flow{if} ((tm\_status = \call{\_xbegin}()) == \const{\_XBEGIN\_STARTED}) \{} \\
		\texttt{~~~~~~~~\flow{if} (\call{hle\_spinlock\_isfree}(lock)) \flow{return};} \\
		\texttt{~~~~~~~~\call{\_xabort}(\const{0xff});} \\
		\texttt{~~~~\} \flow{else} \{} \\
		\texttt{~~~~~~~~\ccomment{// ... retrying \&c abbreviated for brevity ...}} \\
		\texttt{~~~~~~~~\call{hle\_spinlock\_acquire}(lock);} \\
		\texttt{~~~~\}} \\
		\texttt{\}} \\
		\texttt{\ctype{void} \call{rtm\_spinlock\_release}(\ctype{spinlock\_t} *lock)\{} \\
		\texttt{~~~~\flow{if} (\call{hle\_spinlock\_isfree}(lock)) \{} \\
		\texttt{~~~~~~~~\call{\_xend}();} \\
		\texttt{~~~~\} \flow{else} \{} \\
		\texttt{~~~~~~~~\call{hle\_spinlock\_release}(lock);} \\
		\texttt{~~~~\}} \\
		\texttt{\}} \\
		\end{tabular}
	\end{center}
	% TODO describe
	\caption{Code from {\tt spinlock-rtm.c}, modified only to remove unrelated logic for brevity,
		showing the performance bug Landslide found in {\tt lock\_fast}({\tt spinlock}).
		The {\tt isfree()} routine uses an atomic read-and-write operation where just a read would suffice,
		which leads to superfluous memory conflicts in the transactional path
		(seen at both of its callsites below).
	}
	\label{fig:spinlockbug}
\end{figure}

Note that in the AVL tree bug,
the code's author was the very same person who proposed the protocol in Figure~\ref{fig:htm-fixed},
yet still got it wrong once, having to write it out by hand throughout both data structures.
This motivates the need for model checking such programs,
no matter how much of a concurrency expert the author may be.
It also suggests HTM primitives should be encapsulated behind higher-level abstractions,
such as lock elision \cite{lock-elision}
or a simple spinlock \cite{spinlock-rtm-github},
which can be verified in isolation with smaller state spaces
then trusted in turn when checking their client programs \cite{dbug-phdthesis}.
\sect{\ref{sec:tm-verif}} explores this further.

Regarding the spinlock bug,
as HTM is fundamentally a performance-minded concurrency extension,
users are likely to care about more probabilistic properties of their code,
such as requiring a transaction abort rate below a certain threshhold
owing to the nature of its workload.
Landslide cannot in general test for performance degradation bugs,
because all interleavings are equal in Landslide's eyes, and probability is no object.
However, {\tt lock\_fast} illustrates that model checking can still check some interesting performance properties
as long as the element of probability can be removed.
Future work may attempt to verify a wider range of performance properties,
but with a hybrid approach between model checking and what other technique is not yet known.

\subsection{Performance}

Quicksand's ability to find bugs in fewer distinct interleavings (i.e., overall smaller state spaces)
does not necessarily correlate with better performance in terms of CPU-time.
Comparing Table~\ref{tab:buges}'s trends
to the break-even point in Quicksand's evaluation (\sect{\ref{sec:quicksand-eval}}),
most of these tests are too small for its approach to pay off,
% TODO: re check this after re running
with {\tt swapbug}(3,1) and {\tt avl\_insert} as its notable wins.
While plenty more wins were observed in \sect{\ref{sec:quicksand-eval}},
%an upper bound also exists:
%in {\tt swapbug}(3,2),
%not even the {\em minimal} state space was completed in time.
this suggests
future MCs could prioritize state spaces using not just size estimation
but a hybrid approach also conisdering state space maximality and preemption bounds \cite{chess-icb}
to soften the trade-off
both for smaller tests and for verification.

\subsection{Verification}
\label{sec:tm-verif}

Landslide proved the following tests correct.
With no bugs to find, comparing the different testing modes against each other is meaningless;
I simply present their state space sizes and runtime
(measured under {\tt -M})
in Table~\ref{tab:verifs}.

\begin{table}[h]
	\begin{center}
		\footnotesize
		\begin{tabular}{cc|r|r}
			& & \cpu{\bf cpu (s)} & \ints{\bf SS size} \\
			\bf test & \bf params & \em (or \ETAdag{\bf \em ETA}) & \em (or \ETAdag{\bf \em est.}) \\
			\hline
			\hline
			{\tt htm2}
			& 2,1 & \cpu{16.30}	& \ints{22}	\\
			& 2,2 & \cpu{69.80}	& \ints{1970}	\\
			& 2,3 &\cpu{3510.00}	& \ints{104914}	\\
			%& 2,4 &	\ETAdag{...}	& \ETAdag{...}	\\
			& 3,1 &\cpu{41.07}	& \ints{941}	\\
			& 3,2 &	\ETAdag{todo}	& \ETAdag{todo}	\\
			& 4,1 & \cpu{12699.65}	& \ints{344240}	\\
			\hline
			{\tt counter}
			& 2,1 & \cpu{5.40}	& \ints{10}	\\
			& 2,2 & \cpu{10.91}	& \ints{190}	\\
			& 2,3 & \cpu{134.21}	& \ints{3970}	\\
			& 2,4 & \cpu{3009.40}	& \ints{86950}	\\
			& 3,1 & \cpu{8.30}	& \ints{120}	\\
			& 3,2 & \cpu{2038.90}	& \ints{60606}	\\
			& 4,1 & \cpu{101.95}	& \ints{3006}	\\
			\hline
			{\tt swap}
			& 2,1 & \cpu{38.79}	& \ints{99}	\\
			& 2,2 & \cpu{10299.27}	& \ints{233396}	\\
			& 3,1 & \ETAdag{todo} & \ETAdag{todo}	\\
			\hline
			{\tt avl\_insert}
			& 2,1 & \cpu{491.98}	& \ints{15056}	\\
			\hline
			{\tt avl\_fixed}
			& 2,1 & \cpu{572.49} 	& \ints{20420} 	\\
			& 2,2 & \ETAdag{todo}	& \ETAdag{todo} 	\\
			& 3,1 & \ETAdag{todo}	& \ETAdag{todo}	\\

			\hline
			{\tt map\_basic}
			& 2,1 & \ETAdag{todo} & \ETAdag{todo} \\
			\hline
			{\tt map\_basicer}
			& 2,1 & \cpu{36.17}& \ints{426}	\\
			& 2,2 & \ETAdag{todo} & \ETAdag{todo} \\ % currently running
			& 3,1 & \ETAdag{todo} & \ETAdag{todo} \\
		\end{tabular}
	\end{center}
	\caption{Transactional tests verified (or not) by Landslide.
		Run with {\tt landslide -M -X}, i.e., retry aborts enabled, different xabort codes not distinguished.
		State space estimates measured after a timeout of 10 hours.
		}
	\label{tab:verifs}
\end{table}

% TODO rephrase the outdated para from sigbovik here,
% probably after rerunning expts

\subsection{Reduction}

While verifying the soundness of Figure~\ref{fig:htm-fixed}'s atomicity protocol is a positive result,
the previous section's attempts at several larger real-world programs quickly suffered exponential explosion
for even small thread/iteration counts,
in one case failing to make any guarantee about even the minimum possible test configuration.
I explore two possible mitigation approaches.

\subsubsection{Abstraction reduction}

Visual inspection of the AVL tree and separate-chaining map implementations \cite{tm-benchmark-cmu},
after correcting the former's atomicity bug (Figure~\ref{fig:avlbug}),
reveals that every use of HTM followed exactly the same pattern,
running identical data structure logic in both the transactional and abort paths,
as though HTM were merely a mutual exclusion lock with fancy performance characteristics.
% this citation is a lil problematic
Prior work \cite{dbug-phdthesis} proposed {\em abstraction reduction},
in which the user identifies program components that can be separated by a well-understood API,
then tests each one against the API individually,
effectively turning multiplicative state space size factors into additive ones.

In this case, I split the lock-like HTM use
and the mutually-exclusive data structure code
into separate tests,
{\tt lock}, which checks the use of HTM guarantees mutual exclusion,
and {\tt avl\_mutex}/{\tt map\_mutex},
which replace the open-coded HTM use with an already-trusted P2 mutex.
{\tt lock\_fast},
a bonus test,
checks the transactional lock's performance by asserting that
its internal logic won't trigger conflict aborts
even when the client's accesses are independent.
Figure~\ref{fig:htm-lock} shows the core logic of these tests.
Finally, I parameterized them over how the lock was implemented:
{\tt spinlock} ``real-world'' code from \cite{spinlock-rtm-github},
{\tt spin\_fixed} the same with the performance bug from \sect{\ref{sec:tm-eval-bugs}} fixed,
and {\tt mutex} using Landslide-annotated P2 mutexes (as the AVL and map implementations do).

\begin{figure}[h]
	% TODO: syn hi
	\begin{center}
	\begin{tabular}{p{0.47\textwidth}p{0.5\textwidth}}
		\begin{tabular}{l}
			\texttt{\ctype{static int} num\_in\_section = 0;} \\
			\texttt{\flow{for} (\ctype{int} i = \const{0}; i < \const{NITERS}; i++) \{} \\
			\texttt{~~~~\call{rtm\_spinlock\_acquire}(\&lock);} \\
			\texttt{~~~~num\_in\_section++;} \\
			\texttt{~~~~\flow{if} (!\call{\_xtest()})} \\
			\texttt{~~~~~~~~\call{thr\_yield}(\const{-1});} \\
			\texttt{~~~~\flow{assert}{}(num\_in\_section == \const{1}); } \\
			\texttt{~~~~num\_in\_section--;} \\
			\texttt{~~~~\call{rtm\_spinlock\_release}(\&lock);} \\
			\texttt{\}} \\
		\end{tabular}
		&
		\begin{tabular}{ll}
			\texttt{\flow{for} (\ctype{int} i = \const{0}; i < \const{NITERS}; i++) \{} \\
			\texttt{~~~~\call{rtm\_spinlock\_acquire}(\&lock);} \\
			\texttt{~~~~\flow{assert}{}(\call{\_xtest}());} \\
			\texttt{~~~~\call{rtm\_spinlock\_release}(\&lock);} \\
			\texttt{\}} \\
		\end{tabular}
		\\
		\\
		(a) {\tt lock}(), tests mutual exclusion.
		&
		(b) {\tt lock\_fast}(), tests for no spurious aborts.
	\end{tabular}
	\end{center}
	\caption{Abstraction reduction test cases.}
	\label{fig:htm-lock}
\end{figure}

Table~\ref{tab:verifs2} shows the new resulting levels of verification
Landslide reached before the same 10-hour timeout.
Provided that one trusts the {\tt lock} tests correctly check the desired properties,
and that open-coding hadn't introduced any new bugs (such as Figure~\ref{fig:avlbug}'s),
the benefit is clear:
the data structure tests suddenly reach unprecedented thread/iteration counts with ease,
their state space growth now defined only by their internal conflicts from tree rebalancing, map resizing, and so on.
In total,
summing the testing times of {\tt lock}({\tt mutex})$(K,N)$ and {\tt avl\_mutex}$(K,N)$
produces the same verification as {\tt avl\_fixed}$(K,N)$ far more cheaply.
Furthermore, {\tt lock}'s verification can be reused,
whereas {\tt avl\_fixed} and {\tt map\_basic} effectively duplicated the mutex verification between them.
Note also the impact that fixing Figure~\ref{fig:spinlockbug}'s performance bug
(changing a read+write to a read only)
had on even the correctness tests:
{\tt lock}({\tt spin\_fixed})'s state spaces were reduced by more than half compared to {\tt lock}({\tt spinlock}).

\begin{table}[h]
	\begin{center}
		\footnotesize
		%\begin{tabular}{cc}
		%\begin{tabular}{cc||r|r||r|r}
		%	& & \multicolumn{2}{c||}{\bf DPOR} & \multicolumn{2}{c}{\bf DPOR + abort sets} \\
		%	& & \cpu{\bf cpu (s)} & \ints{\bf SS size}
		%	  & \cpu{\bf cpu (s)} & \ints{\bf SS size} \\
		%	\bf test & \bf params & \em (or \ETAdag{\bf \em ETA}) & \em (or \ETAdag{\bf \em est.})
		%	  & \em (or \ETAdag{\bf \em ETA}) & \em (or \ETAdag{\bf \em est.}) \\
		\begin{tabular}{cc||r|r}
			& & \cpu{\bf cpu (s)} & \ints{\bf SS size} \\
			\bf test & \bf params & \em (or \ETAdag{\bf \em ETA}) & \em (or \ETAdag{\bf \em est.}) \\
			\hline
			\hline
			{\tt lock}
			& 2,1 & \cpu{14.23} & \ints{41} \\
			({\tt spinlock})
			& 2,2 & \cpu{4944.37} & \ints{38544} \\
			% if lock(spin_fixed) will time out on 2,3 and 3,2, this is guaranteed to aswell
			% & 2,3 & \\ % \cpu{0} & \ints{0} \\
			& 3,1 & \cpu{13649.32} & \ints{97462} \\
			% & 3,2 & \\ % \cpu{0} & \ints{0} \\
			% new spinlock test resulce - htm_spinlock
			% (2,1) without isfree fix
			%%%% total time elapsed: 9s
			%%%% [JOB 4] COMPLETE (41 interleavings tested; 3s elapsed)
			%%%% total CPU time consumed: 14s (14231661 usecs) (core saturation: 37%)
			% (2,2) without isfree fix
			%%%% total time elapsed: 1h 22m 20s
			%%%% [JOB 4] COMPLETE (38544 interleavings tested; 1h 22m 13s elapsed)
			%%%% total CPU time consumed: 1h 22m 24s (4944373889 usecs) (core saturation: 25%)
			% (3,1) without isfree fix
			% in progress but smth like
			% [JOB 4] progress: 39726/83689 brs (47.468314%), ETA 3h 27m 10s (elapsed 1h 30m 21s)
			\hline
			{\tt lock}
			& 2,1 & \cpu{12.26} & \ints{31} \\
			({\tt spin\_fixed})
			& 2,2 & \cpu{1662.21} & \ints{15076} \\
			& 2,3 & \ETAdag{3d 20h} & \ETAdag{2003906} \\
			& 3,1 & \cpu{2488.95} & \ints{24970} \\
			& 3,2 & \ETAdag{207d 20h} & \ETAdag{6514618} \\
			& 4,1 & \\ % TODO
			% (2,1) with isfree fix
			%%%% total time elapsed: 10s
			%%%% [JOB 4] COMPLETE (31 interleavings tested; 2s elapsed)
			%%%% total CPU time consumed: 13s (13256415 usecs) (core saturation: 30%)
			% (2,2) with isfree fix
			% total time elapsed: 27m 39s
			% [JOB 4] COMPLETE (15076 interleavings tested; 27m 28s elapsed)
			% total CPU time consumed: 27m 42s (1662213886 usecs) (core saturation: 25%)
			% (2,3)
			% [JOB 4] progress: 242266/2003906 brs (12.089688%), ETA 3d 20h 2m 12s (elapsed 9h 59m 49s)
			% [JOB 4] TIMED OUT (12.089688%; ETA 3d 20h 2m 12s)
			% (3,1) with isfree fix
			% hot start confirmed
			%%%% total time elapsed: 41m 26s
			%%%% [JOB 4] COMPLETE (24790 interleavings tested; 41m 18s elapsed)
			%%%% total CPU time consumed: 41m 28s (2488953483 usecs) (core saturation: 25%)
			% (3,2)
			% [JOB 4] progress: 187522/6514618 brs (2.878480%), ETA 207d 20h 23m 10s (elapsed 9h 59m 51s)
			% [JOB 4] TIMED OUT (2.878480%; ETA 207d 20h 23m 10s)
			\hline
			{\tt lock}
			& 2,1 & \cpu{9.42} & \ints{22}	\\
			({\tt mutex})
			& 2,2 & \cpu{44.07} & \ints{1208}	\\
			& 2,3 & \cpu{2647.38} & \ints{71136}	\\
			& 2,4 & \ETAdag{2d 1h} & \ETAdag{3223055}	\\
			& 3,1 & \cpu{42.11} & \ints{1148}	\\
			& 3,2 & \ETAdag{11d 4h} & \ETAdag{4793666}	\\
			& 4,1 & \cpu{5586.37} & \ints{159664}	\\
			%%%%%%%%%%% htm_mutex
			%%%% -- nb. these numbers were obtained with a oops-exploded state space
			%%%% from a misplaced assert on hm->held
			% % 2,1
			% total time elapsed: 6s
			% [JOB 2] COMPLETE (24 interleavings tested; 3s elapsed)
			% total CPU time consumed: 9s (9642348 usecs) (core saturation: 35%)
			% % 2,2
			% total time elapsed: 1m 20s
			% [JOB 2] COMPLETE (2672 interleavings tested; 1m 16s elapsed)
			% total CPU time consumed: 1m 23s (83001932 usecs) (core saturation: 25%)
			% % 2,3
			% total time elapsed: 2h 3m 45s
			% [JOB 2] COMPLETE (220338 interleavings tested; 2h 3m 40s elapsed)
			% total CPU time consumed: 2h 3m 47s (7427873599 usecs) (core saturation: 25%)
			% % 3,1
			% total time elapsed: 51s
			% [JOB 2] COMPLETE (1594 interleavings tested; 47s elapsed)
			% total CPU time consumed: 54s (54572810 usecs) (core saturation: 26%)
			%%%% -- nb. better version
			% % 2,1
			% total time elapsed: 6s
			% [JOB 2] COMPLETE (22 interleavings tested; 2s elapsed)
			% total CPU time consumed: 9s (9415448 usecs) (core saturation: 34%)
			% % 2,2
			% total time elapsed: 40s
			% [JOB 2] COMPLETE (1208 interleavings tested; 37s elapsed)
			% total CPU time consumed: 44s (44068381 usecs) (core saturation: 27%)
			% % 2,3
			% total time elapsed: 44m 4s
			% [JOB 2] COMPLETE (71136 interleavings tested; 44m 0s elapsed)
			% total CPU time consumed: 44m 7s (2647380230 usecs) (core saturation: 25%)
			% % 2,4
			% [JOB 2] progress: 957248/3223055 brs (29.700013%), ETA 2d 1h 24m 44s (elapsed 9h 59m 56s)
			% total time elapsed: 10h 0s
			% [JOB 2] TIMED OUT (29.700013%; ETA 2d 1h 24m 44s)
			% total CPU time consumed: 10h 3s (36003560403 usecs) (core saturation: 25%)
			% % 3,1
			% total time elapsed: 38s
			% [JOB 2] COMPLETE (1148 interleavings tested; 35s elapsed)
			% total CPU time consumed: 42s (42108005 usecs) (core saturation: 27%)
			% % 3,2
			% [JOB 2] progress: 920684/4793666 brs (19.206260%), ETA 11d 4h 17m 50s (elapsed 9h 59m 56s)
			% total time elapsed: 10h 0s
			% [JOB 2] TIMED OUT (19.206260%; ETA 11d 4h 17m 50s)
			% total CPU time consumed: 10h 3s (36003199876 usecs) (core saturation: 25%)
			% % 4,1
			% total time elapsed: 1h 33m 3s
			% [JOB 2] COMPLETE (159664 interleavings tested; 1h 32m 59s elapsed)
			% total CPU time consumed: 1h 33m 6s (5586369677 usecs) (core saturation: 25%)
			\hline
			{\tt lock\_fast}
			& 2,1 & \cpu{2.96} & \ints{1}	\\
			({\tt spin\_fixed})
			& 9,9 & \cpu{4.33} & \ints{1}	\\
			\hline
			{\tt lock\_fast}
			& 2,1 & \cpu{3.05} & \ints{1}	\\
			({\tt mutex})
			& 9,9 & \cpu{4.38} & \ints{1}	\\
			%\end{tabular}
			%&
			%\begin{tabular}{cc||r|r}
			%& & \cpu{\bf cpu (s)} & \ints{\bf SS size} \\
			%\bf test & \bf params & \em (or \ETAdag{\bf \em ETA}) & \em (or \ETAdag{\bf \em est.}) \\
			\hline
			\hline
			{\tt avl\_mutex}
			& 2,1 & \cpu{2.49} & \ints{3} \\
			& 2,2 & \cpu{2.44} & \ints{3} \\
			& 2,3 & \cpu{2.45} & \ints{3} \\
			& 2,4 & \cpu{2.41} & \ints{3} \\
			& 3,1 & \cpu{2.69} & \ints{12} \\
			& 3,2 & \cpu{2.67} & \ints{12} \\
			& 3,3 & \cpu{2.76} & \ints{13} \\
			& 3,4 & \cpu{2.74} & \ints{13} \\
			& 4,1 & \cpu{4.03} & \ints{60} \\
			& 4,2 & \cpu{4.34} & \ints{66} \\
			& 4,3 & \cpu{4.53} & \ints{71} \\
			& 4,4 & \cpu{4.75} & \ints{73} \\
			%%%% avl_mutex
			% % 2,1
			% [JOB 0] COMPLETE (3 interleavings tested; 2s elapsed)
			% total CPU time consumed: 2s (2489515 usecs) (core saturation: 24%)
			% % 2,2
			% [JOB 0] COMPLETE (3 interleavings tested; 1s elapsed)
			% total CPU time consumed: 2s (2443451 usecs) (core saturation: 24%)
			% % 2,3
			% [JOB 0] COMPLETE (3 interleavings tested; 1s elapsed)
			% total CPU time consumed: 2s (2448127 usecs) (core saturation: 24%)
			% % 2,4
			% [JOB 0] COMPLETE (3 interleavings tested; 1s elapsed)
			% total CPU time consumed: 2s (2407903 usecs) (core saturation: 24%)
			% % 3,1
			% [JOB 0] COMPLETE (12 interleavings tested; 2s elapsed)
			% total CPU time consumed: 2s (2685913 usecs) (core saturation: 24%)
			% %3,2
			% [JOB 0] COMPLETE (12 interleavings tested; 2s elapsed)
			% total CPU time consumed: 2s (2666826 usecs) (core saturation: 24%)
			% % 3,3
			% [JOB 0] COMPLETE (13 interleavings tested; 2s elapsed)
			% total CPU time consumed: 2s (2761157 usecs) (core saturation: 24%)
			% % 3,4
			% [JOB 0] COMPLETE (13 interleavings tested; 2s elapsed)
			% total CPU time consumed: 2s (2738169 usecs) (core saturation: 24%)
			% % 4,1
			% [JOB 0] COMPLETE (60 interleavings tested; 3s elapsed)
			% total CPU time consumed: 4s (4034411 usecs) (core saturation: 24%)
			% % 4,2
			% [JOB 0] COMPLETE (66 interleavings tested; 3s elapsed)
			% total CPU time consumed: 4s (4338442 usecs) (core saturation: 24%)
			% % 4,3
			% [JOB 0] COMPLETE (71 interleavings tested; 4s elapsed)
			% total CPU time consumed: 4s (4526641 usecs) (core saturation: 24%)
			% % 4,4
			% [JOB 0] COMPLETE (73 interleavings tested; 4s elapsed)
			% total CPU time consumed: 4s (4752864 usecs) (core saturation: 24%)
			\cline{1-4}
			{\tt map\_mutex}
			& 2,1 & \\ %\cpu{0} & \ints{0} \\
			& 2,2 & \\ %\cpu{0} & \ints{0} \\
			& 2,3 & \\ %\cpu{0} & \ints{0} \\
			& 3,1 & \\ %\cpu{0} & \ints{0} \\
			& 3,2 & \\ %\cpu{0} & \ints{0} \\
		\end{tabular}
		%\end{tabular}
	\end{center}
	\caption{Continuation of Table~\ref{tab:verifs}.
		Demonstrates abstraction reduction \cite{dbug-phdthesis}
		on the {\tt avl\_fixed} and {\tt map\_basic} tests
		by verifying HTM mutex implementations separately.
		Run with {\tt landslide -M -X -A -S}, i.e.,
		conflict and explicit aborts tested separately,
		and retry aborts suppressed
		(both lock implementations include a retry loop).
		The reduced {\tt avl\_mutex} and {\tt map\_mutex} are, of course, no longer transactional.
		%% i mean ok xabort sets doesn't really help for ANY test tho
		%so the abort sets optimization does not apply to them.
		}
	\label{tab:verifs2}
\end{table}

\subsubsection{Abort set reduction}

% TODO

(\sect{\ref{sec:tm-abortsets}}).

% as for the sleep-set-alike optimization,
% we see it would provide 25% reduction in the example (from thesprop)
% however, evaluating its potential impact in general is tantamount to implementing it.

%%%% map mutex
% % TODO - bc of this weird nonlinear behaviour around 2,1+, yo ushoud prob try 2,2 for map_basic as well
% % 2,1
% total time elapsed: 19s
% [JOB 7] COMPLETE (57 interleavings tested; 5s elapsed)
%        PPs: { 'mutex_lock' 'mutex_unlock' 'DR @ 0x100053b' [NONDET] 'DR @ 0x1000552' [NONDET] 'DR @ 0x1000521' [NONDET]
%        'DR @ 0x100055d' [NONDET] 'DR @ 0x100055a' [NONDET] 'DR @ 0x1000916' [NONDET] 'DR @ 0x1000919' [NONDET] }
% total CPU time consumed: 31s (31198379 usecs) (core saturation: 39%)
% % 2,2
% total time elapsed: 3s
% [JOB 0] COMPLETE (6 interleavings tested; 2s elapsed)
%        PPs: { 'mutex_lock' 'mutex_unlock' }
% =========================
% progress report thr exiting
% total CPU time consumed: 3s (3192831 usecs) (core saturation: 24%)
% % 2,3
% ==== PROGRESS REPORT ====
% total time elapsed: 3s
% [JOB 0] COMPLETE (6 interleavings tested; 2s elapsed)
%        PPs: { 'mutex_lock' 'mutex_unlock' }
% =========================
% progress report thr exiting
% total CPU time consumed: 3s (3240367 usecs) (core saturation: 24%)
% 2,4
% 3,1
% 3,2


% commentary -- writing this test case well requires expert knowledge of noobs (of writing landslide friendly tests)
% - placing the asserts well, using sync_test_and_set and asserting on the result rather than having a separate read,
%   has an impact on the state space, as seen above
% - design of critical_section and how it uses xtest to yield only in failure path, and why that's justified


% other one to test -
% https://github.com/vamsikc/leveldb-tsx/blob/master/ext/xsync/include/scope.hpp

% "Although we skipped implementing it to begin with, the sheer size of this reduction also
% reflects on the numbers one might see betwee the original reduction in theorem so-and-so that lets us
% treat transactional success paths as mutually-exclusive (and, once started, abort-proof) to begin with"
% this sentence is a complete mess....
