\chapter{Education}
\label{chap:education}

\inspirationalquote{Knowing the students might one day
%find a way to
fix their concurrency bugs...
it fills you with determination.}{Undertale (paraphrased)}

Concurrency is taught in as many different ways as there are
systems programming classes at universities which teach the subject.
Yet one thing they all have in common is presenting the concurrency bug
as some elusive menace,
against which humanity's best weapon is mere random stress testing.
This chapter will prove stateless model checking's mettle as a better alternative in the educational theatre.

While the previous chapter demonstrated Landslide's bug-finding power
compared to prior MC techniques in a controlled environment,
whether it offers pedagogical merit in the hands of students and/or TAs is a separate question.
And while my MS thesis~\cite{landslide} showed that students
could annotate P3 Pebbles kernels and thence use Landslide to debug them,
the annotations alone required 2 hours of effort on average per user,
meaning the only students who could benefit were the ones already succeeding enough to have such free time.
% TODO: sect ref
Since then, I have extended Landslide with a fully-automatic instrumentation process
for Pebbles thread libraries (P2s) and Pintos kernels
to improve its accessibility.

I have run several user studies in the Operating Systems classes
at Carnegie Mellon University (CMU), University of Chicago (U. Chicago), and Penn State University (PSU),
wherein students get to use Landslide to find and diagnose their own bugs during the semester.
% TODO: put section refs
At CMU, I recorded logs and code snapshots as students used Landslide during P2.
At CMU and PSU, I surveyed students on their experience after submitting their Landslide-debugged P2s.
At U. Chicago, I collaborated with a TA to check submitted Pintos kernels,
then returned any resulting bug reports to students and likewise surveyed them on the quality of the diagnostic output.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Pebbles}

This section presents the user studies done in
%CMU's 15-410 and PSU's \psuos classes,
%in semesters Fall 2015 to Spring 2018 and in Spring 2018 alone, respectively,
%taught by David Eckhardt and Timothy Zhu, respectively.
CMU's 15-410 in semesters Fall 2015 to Spring 2018,
taught by David Eckhardt,
and in PSU's \psuos in Spring 2018,
taught by Timothy Zhu.
In both cases the instructors assisted to introduce me during the guest lecture
%(see below)
and to distribute the recruiting emails;
TAs were not involved.
The in-house user study has CMU IRB approval under study number STUDY2016\_00000425,
and the external user study under STUDY2017\_00000429.

% Possible experiment questions
% compare e.g. use after free bug reporce from OOPSLA data set, to P2 grade files, see who has thread exit uafs
% is landslide better at finding thread uafs than TAs
% same Q for other stuff.. (expect paraguay answer to be "no", explain why)

\subsection{Recruiting}
\label{sec:education-pebbles-recruiting}

Since the Spring 2015 semester I have given a guest lecture in 15-410
to recruit students to participate in the user study.
The 50-minute lecture is given 1 week into the 2.5-week-long P2 project,
approximately when the students should be getting child threads running in {\tt thr\_create()}
and experiencing concurrency bugs for the first time.
It introduces the research subject abstractly
using an example ``Paradise Lost'' bug from a previous lecture \cite{paradise-lost},
explains how Landslide works concretely,
shows a short demo of effortlessly using Landslide to find the example bug,
and provides the necessary IRB legalese about the risks and benefits of participation.
The most recent lecture slides are available on the course website at
\url{http://www.cs.cmu.edu/~410-s18/lectures/L14_Landslide.pdf},
and all semesters' editions at
\url{https://github.com/bblum/talks/tree/master/landslide-lecture}.

The PSU version of the lecture
%was given in Spring 2018, and
is available at
\url{http://www.contrib.andrew.cmu.edu/~bblum/psu-lecture.pdf}
as well as under the github link above.
Being a 70-minute lecture slot rather than 50, I extended the demo to
both find and (attempt to) verify a fix for two bugs:
one a simple data race and the other the more complicated Paradise Lost bug as above.
After finding each bug, I demonstrated using Landslide on a fixed version of the code
to show how it proves the test case correct by completing all state spaces,
or (in the case of Paradise Lost) suffers an exponentially-exploding state space.
Not that I scientifically measured it or anything,
but this extended demo seemed to help students more clearly understand Landslide's intended workflow,
at the cost of about 10-15 extra minutes of lecture time.

At both schools students then signed up using a Google form I emailed them,
which upon completion linked them to the Landslide user guide,
which is available at
\url{http://www.contrib.andrew.cmu.edu/~bblum/landslide-guide-p2.pdf}
(CMU version)
and
\url{http://www.contrib.andrew.cmu.edu/~bblum/landslide-guide-psu.pdf}
(PSU version)
and
\url{https://github.com/bblum/talks/tree/master/irb}
(both versions).

\subsection{Automatic instrumentation}
\label{sec:education-pebbles-instrumentation}

As described in \sect{\ref{sec:landslide-setup}},
all setup from the user's point of view is handled through the {\tt p2-setup.sh} script%
\footnote{PSU's version is called {\tt psu-setup.sh};
in this section, unless otherwise noted, {\tt p2-setup.sh} will refer to both.
}.
It, its helper scripts (\sect{\ref{sec:landslide-glue}}),
and the {\tt landslide} script itself contain several checks to prevent
studence
from accidentally misusing Landslide in ways that could produce mysterious crashes, false bug reports, and so on
(the need for each one, as the reader might imagine, discovered through bitter experience).
These include:
\begin{itemize}
	\item {\tt p2-setup.sh} checks if the directory argument correctly points at the top-level P2 basecode directory
		rather than any subdirectories such as {\tt user/libthread/}.
	\item {\tt check-need-p2-setup-again.sh} checks if any source files in the original P2 source directory
		(the argument supplied to {\tt p2-setup.sh}),
		in case the student hoped to fix some bug and verify their fix but forgot to re-run the setup script.
	\item {\tt landslide} checks the supplied test name matches one of the endorsed Landslide-friendly tests
		(students love trying to run Landslide with {\tt racer}, {\tt largetest},
		or even the string {\tt OPTIONS}).
	\item {\tt landslide} checks if any other instance of itself is simultaneously running in the same directory,
		and if so, refuses to do so and advises the student
		to {\tt git clone} the repository afresh for simultaneous use%
		\footnote{This is ironically implemented with a non-atomic lock file
		and should really be using {\tt flock} instead.
		}.
\end{itemize}
\vspace{1em}

\noindent Landslide also includes several P2-specific instrumentations and features to cope with various student irregularities:
\begin{itemize}
	\item Quicksand emits different combinations of {\tt within\_function}/{\tt without\_function} directives
		for Landslide depending on the name of the test.
		For example, for {\tt paradise\_lost} Landslide will not preempt in a function named {\tt critical\_section()},
		which the test case uses to protect an internal counter used to detect the bug;
		and it will not preempt in any of the {\tt thr\_*()} thread library API functions
		for tests intended to target just the concurrency primitives.
		In future work this could be improved as annotations to be placed inside the test case code itself.
	\item Landslide finds ad-hoc synchronization patterns which students often open-code,
		rather than using the prescribed synchronization API,
		such as {\tt while (!flag) yield();} or {\tt while (xchg(...)) continue;},
		and treats them as synchronization points as described in \sect{\ref{sec:landslide-blocking}}.
	\item Landslide finds ``too suspicious'' spinwait-loops in the students' mutex implementations
		which are neither yield- nor {\tt xchg}-loops (as described above),
		which would ordinarily be classified as infinite loop bugs,
		and reports them with a suggestive message ({\tt undesirable\_loop\_html()} in {\tt landslide.c})
		referring them to the appropriate lecture material
		\cite{synchronization-2}.
	\item The {\tt landslide} wrapper script logs the time and command-line options of invocation
		and captures a snapshot of the student code and results of the test and saves them to AFS after each run.
		% HURDLE_VIOLATION -- no, this is for p3 (context switcher) features only
\end{itemize}

\subsection{Test cases}
\label{sec:education-pebbles-tests}

Landslide ships with several ``approved'' test cases,
i.e., programs copied from, derived from, or at least vaguely resembling
the tests distributed with P2,
which I curated to produce concurrent behaviour suitable for stateless model checking.
Some tests are crafted to target specific bugs which,
from personal experience as a TA, are common in many student submissions;
others are crafted to exercise generally concurrency-heavy code paths and uncover any number of unforeseen problems.
Many use some of the features/annotations described in \sect{\ref{sec:landslide-testcases}}.

\subsubsection{Test case list}

The following tests were released to CMU students:
\begin{itemize}
	\item {\tt broadcast\_test}:
		Tests the {\tt cond\_broadcast()} signalling path with a single waiter.
	\item {\tt mutex\_test}:
		Tests student mutexes under 2 threads with 2 iterations
		(the 2nd iteration serves to expose problems with {\tt mutex\_unlock()} as well as {\tt mutex\_lock()}).
		This test uses the {\tt TESTING\_MUTEXES}
		described in \sect{\ref{sec:landslide-staticconfig}}
		to enable data-race preemption points within the mutex implementation.
	\item {\tt paradise\_lost}:
		Written for the sake of the Landslide lecture demo
		(\sect{\ref{sec:education-pebbles-recruiting}}).
		Tests for the Paradise Lost bug by attempting to break mutual exclusion.
	\item {\tt paraguay}:
		Copied directly from the P2 test suite;
		tests for proper handling of seemingly ``spurious'' wakeups in {\tt cond\_wait()}.
		Written by Michael Sullivan.
	\item {\tt rwlock\_downgrade\_read\_test}:
		Copied directly from the P2 test suite;
		tests for mutually-exclusive and deadlock-free {\tt rwlock\_downgrade()}.
		Written by me (as a TA).
	\item {\tt thr\_exit\_join}:
		Copied directly from the P2 test suite;
		tests for a variety of problems between {\tt thr\_exit()} and {\tt thr\_join()},
		but especially for memory issues pertaining to stack deallocation.
\end{itemize}
\vspace{1em}

\noindent The following tests were released to PSU students, in addition to the ones above:
\begin{itemize}
	\item {\tt atomic\_compare\_swap}:
		Tests the {\tt cmpxchg} assembly function for being properly atomic.
		Uses the {\tt magic\_*} global variables described below, and invokes {\tt vanish()} directly,
		to avoid requiring the student to implement {\tt thr\_join()}/{\tt thr\_exit()}
		before being able to run this test.
	\item {\tt atomic\_exchange}:
		As above for {\tt xchg}.
	\item {\tt atomic\_fetch\_add}:
		As above for {\tt xadd}.
	\item {\tt atomic\_fetch\_sub}:
		As above for {\tt xadd}.
	\item {\tt broadcast\_two\_waiters}:
		As {\tt broadcast\_test}, but uses two waiting threads to ensure both get signalled.
\end{itemize}
\vspace{1em}

\noindent The tests can all be viewed at \url{https://github.com/bblum/landslide/tree/master/pebsim/p2-basecode/410user/progs}.

\subsection{Survey}
\label{sec:education-survey-pebbles}

Starting in Fall 2017, I sought to gauge the students' personal opinions on their experience with Landslide,
in addition to simply counting
from the automatic snapshots
how many bugs were found.
Shortly after the P2 submission deadline,
I asked participants to answer several survey questions, reproduced below.
%distributed via email as a Google Doc.

\begin{enumerate}
	\item How many bugs did Landslide help you find in your code? (Please indicate a number.)
	\item How many of the bugs you found with Landslide do you believe you fixed before submitting your project? (You may answer ``all'', ``none'', or a number.)
	\item How many of the bugs you found with Landslide did you verify you had fixed by running Landslide again to make sure the bug was gone? (You may answer ``all'', ``none'', or a number.)
	\item In addition to the bugs Landslide found, did it report anything that you believe was NOT a bug? For example, Landslide printed an execution trace that was actually impossible, or Landlside reported a bug about some behaviour that was actually allowed by the P2 specification. (If so, please describe.)
	\item I found Landslide's debugging output easy to understand.
		(Multiple choice from strongly disagree to strongly agree.)
	\item It's easier to diagnose the root cause of a bug with Landslide than with a stress test (e.g. juggle).
		(Multiple choice from strongly disagree to strongly agree; plus ``Not sure'' and ``Easier for some bugs but harder for others'')
	\item I felt the time I saved by having Landslide to help debug was worth the time it took me to learn how to use Landslide.
		(Multiple choice from strongly disagree to strongly agree.)
	\item I feel that by using Landslide I learned to understand concurrency better.
		(Multiple choice from strongly disagree to strongly agree.)
	\item Suppose after you submitted your project, % "P2" for cmu, "project" for pintos
		we gave you 100 CPU-hours on the cloud provider of your choice to test it. Then we extended the project deadline by a day for you to use the results to fix bugs and get partial credit. How would you divide that CPU time between the staff-provided stress tests and Landslide?
		(Multiple choice: 0/10/.../100 CPU-hours on Landslide, 100/90/.../0 CPU-hours on stress tests.)
	\item If I found out next semester that a friend of mine (or a student in my degree program) were taking OS, I would recommend that they should probably invest some time during the project % "P2" for cmu, as above
		to learn Landslide and try to find bugs with it.
		(Multiple choice from strongly disagree to strongly agree.)
	\item Regarding the previous question, why or why not?
\suspend{enumerate}
\vspace{1em}

\noindent The following questions were served only on the CMU version of the survey.
\resume{enumerate}
	\item Did you answer this survey together with your partner, or on your own while they were busy? (If you both have time for it, please try to submit one survey together.)
		(Multiple choice: together or alone)
	\item Your andrew ID
	\item Your partner's andrew ID (if any)
\suspend{enumerate}
\vspace{1em}

\noindent The following questions were served only on the PSU version of the survey.
\resume{enumerate}
	\item Any feedback on how Landslide's user interface could be improved / made easier to use or understand? (setup process, messages printed while running, or the execution trace / stack traces emitted after a bug is found?)
	\item Your PSU username
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Pintos}

This section presents the user study done in U. Chicago's \uchos class in the Fall 2017 semester,
taught by Haryadi Gunawi.
Kevin Zhao, the TA, assisted to run Landslide on student submissions
and to distribute recruiting materials and testing results.
The study has CMU IRB approval under study number STUDY2017\_00000429.

\subsection{Recruiting}

For this study students were recruited remotely via email.
After each of the {\em threads} and {\em userprog} project deadlines (\sect{\ref{sec:overview-pintos}}),
\uchos staff sent students an email inviting them to volunteer to receive Landslide's bug reports,
disclaiming that it did not represent part of the official grading process but could help improve their future submissions.

\subsection{Automatic instrumentation}
\label{sec:education-pintos-instrumentation}

As described in \sect{\ref{sec:landslide-setup}},
all setup from the user's point of view is handled through the {\tt pintos-setup.sh} script.
It
and its helper {\tt pebsim/pintos/import-pintos.sh}
perform most of the same sanity checks as listed in \sect{\ref{sec:education-pebbles-instrumentation}},
then applies the patch {\tt annotate-\allowbreak{}pintos.patch}
(plus several more hacks in the script itself)
to insert the {\tt tell\_landslide()} annotations (\sect{\ref{sec:tell-landslide}})
into the student's kernel code.
The following tricks serve to make sure the annotations apply consistently
to (almost) all variations of code that students commonly submit:

\begin{itemize}
	\item Finds the declaration of {\tt ready\_list}, the scheduler runqueue declared by the basecode,
		and detects if the student has modified to be an array of lists rather than a single one.
		If so, defines the length of that array in a macro to be used by {\tt is\_runqueue()}
		(part of the patch described below).
		Either way defines a function {\tt get\_rq\_addr()} to return the address of the (first) list.
	\item Changes the basecode's definition of {\tt TIME\_SLICE} from 4 to 1 (units of timer ticks)
		so Landslide's timer injection will properly drive the context switcher.
	\item Inserts {\tt tell\_landslide\_forking()} into {\tt thread.c}
		(using {\tt sed} rather than the patch, described below,
		because it must go in a function which students have to implement,
		which is likely to disturb the context and make a patch fail).
	\item Adds the new {\tt priority-donate-multiple} test.
	\item Applies the {\tt annotate-pintos.patch} patch to the imported student implementation, which:
	\begin{itemize}
		\item Adds {\tt tell\_landslide\_thread\_on\_rq()}
			and {\tt tell\_landslide\_thread\_off\_rq()}
			annotations
			to {\tt list\_insert()} and {\tt list\_remove()} respectively
			(in {\tt lib/kernel/\allowbreak{}list.c}, which the students don't modify),
			which
			check whether the argument list
			is the scheduler runqueue
			using a helper function {\tt is\_runqueue},
			which in turn uses {\tt get\_rq\_addr()} and {\tt READY\_LIST\_LENGTH} described above.
		\item Modifies the existing {\tt priority-sema} and {\tt alarm-simultaneous} tests to be more Landslide-friendly.
		\item Inserts the {\tt tell\_landslide\_sched\_init\_done()},
			{\tt tell\_landslide\_vanishing()},
			and {\tt tell\_landslide\_thread\_switch()}
			annotations in the appropriate places
			(which the students generally do not modify).
	\end{itemize}
	\item Detects if the student has renamed the {\tt elem} field of the TCB struct,
		and if so renames its use in {\tt is\_runqueue()} (described above) correspondingly.
\end{itemize}

\subsection{Test cases}
\label{sec:education-pintos-tests}

Like the P2 tests, the Pintos test cases are either hand-picked from the provided unit tests,
with an eye for which will produce interesting concurrent behaviour,
or created using a TA's intuition for the most likely student bugs.
The following tests are approved to be Landslide-friendly:

\begin{itemize}
	\item {\tt priority-sema}:
		Modified to be Landslide-friendly from the basecode, for {\em threads}.
		Creates two child threads to wait on a semaphore and signals them.
		Replaces threads with different priorities
		(originally chosen to produce deterministic output which the test checked for)
		with threads of the same priority.
	\item {\tt alarm-simultaneous}:
		Modified to be Landslide-friendly from the basecode, for {\em threads}.
		Creates two child threads which each invoke {\tt timer\_sleep()} for a different amount of time.
		Number of (threads,iterations) reduced from (3,5) to (2,1).
	\item {\tt wait-simple}:
		Unmodified from the basecode's version, for {\em userprog}.
		Userspace process {\tt exec()}s a child process, which immediately exits, and {\tt wait()}s on it.
	\item {\tt priority-donate-multiple}:
		Written by Kevin Zhao, TA at U. Chicago, for {\em threads}.
		Tests for a priority donation race during {\tt lock\_release()}
		in which a thread holding a lock can accidentally keep a contending thread's donated priority
		after finishing releasing it.
\end{itemize}
\vspace{1em}

The (unpatched versions of) the first three tests are available at
\url{https://github.com/Berkeley-CS162/group0/tree/master/pintos/src/tests}.
The fourth test is available at
\url{https://github.com/bblum/landslide/blob/master/pebsim/pintos/priority-donate-multiple.c}.

\subsection{Survey}

Similar to the survey for Pebbles projects
\sect{\ref{sec:education-survey-pebbles}},
I surveyed the Pintos user study participants for their opinions.
Because of the different nature of the user study, of course,
the questions here focus more on the debugging experience than on using Landslide directly.

\begin{enumerate}
	\item How many Landslide bug reports did you receive from course staff? (Please indicate a number.)
	\item Among those bug reports, how many were you able to diagnose and recognize the root cause in your code? (You may answer ``all'', ``none'', or a number.)
	\item Among those bug reports, how many described a behaviour that you believe was NOT a bug? For example, Landslide printed an execution trace that was actually impossible, or Landslide reported a bug about some behaviour that was actually allowed by the Pintos specification. (You may answer ``all'', ``none'', or a number.)
	\item About how much time did you spend interpreting Landslide's debugging output? (Please indicate a number of minutes, or a range if uncertain, e.g. ``30-60 minutes''.)
	\item I found Landslide's debugging output easy to understand.
		(Multiple choice from strongly disagree to strongly agree.)
	\item It's easier to diagnose the root cause of a bug with Landslide than with a stress test (for example {\tt exec-multiple}).
		(Multiple choice from strongly disagree to strongly agree; plus ``Not sure'' and ``Easier for some bugs but harder for others'')
	\item I feel that by interpreting Landslide's debugging output I learned to understand concurrency better.
		(Multiple choice from strongly disagree to strongly agree.)
	\item These kinds of concurrency bugs are important to fix, even though they don't count against my grade.
		(Multiple choice from strongly disagree to strongly agree.)
	\item Suppose after you submitted your pintos, we gave you 100 CPU-hours on the cloud provider of your choice to test it. Then we extended the project deadline by a day for you to use the results to fix bugs and get partial credit. How would you divide that CPU time between the staff-provided stress tests and Landslide?
		(Multiple choice: 0/10/.../100 CPU-hours on Landslide, 100/90/.../0 CPU-hours on stress tests.)
	\item If course staff were to allow students to resubmit updated code after reviewing Landslide bug reports to receive partial credit for each bug that had been fixed, it would be worth my time to try that (even if I could be spending that time working on the next project instead).
		(Multiple choice from strongly disagree to strongly agree.)
	\item If a friend of mine took OS next semester, I would recommend that they should sign up to receive Landslide bug reports during projects in the future.
		(Multiple choice from strongly disagree to strongly agree.)
	\item Regarding the previous question, why or why not?
	\item Your name
	\item Your project partner's name (if applicable)
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Evaluation}

I pose the following evaluation questions.

\begin{itemize}
	\item How many bugs, and of what severities, does Landslide help students find and fix before submitting their code?
	\item Does Landslide use result in higher quality submissions, whether directly for P2, or for subsequent projects as well?
	\item Do students feel the experience is worthwhile, as compared to stress testing?
	\item How well does Landslide apply to operating systems projects outside of CMU?
\end{itemize}
\vspace{1em}

The data set is comprised of Landslide's automatically-generated usage snapshots
from CMU students from semesters Spring 2015 to Spring 2018 inclusive,
% and PSU \psuos from Spring 2018, % no snapshots from psu
CMU's official project grades from same,
CMU students' survey responses and submitted project code from Fall 2017 and Spring 2018
PSU students' survey responses and submitted project code from Spring 2018,
and U. Chicago students' survey responses from Fall 2017.

\subsection{Bug-finding}
\label{sec:education-eval-bugfinding}

\subsection{Student submission quality}

I evaluated Landslide's impact on student submission quality in two ways.
First, I analyzed the overall P2 grades of CMU 15-410 students between Landslide users and non-users,
to see if Landslide helped them submit overall better implementations,
as well as their later overall P3 (kernel project) grades,
to see if Landslide helped them internalize any concurrency debugging skills
that would help write better kernels even without Landslide's aid.
% This experiment produced inconclusive or negative results. % say this?

Second, I studied
several % how many? 3
individual concurrency bugs that I thought Landslide was likely to detect,
all already well-known by 15-410 course staff,
to see if using Landslide correlated with submitting projects absent those bugs.
% results..?

\subsubsection{Impact on grades}

% TODO

\subsubsection{Common bugs}

Based on the results from \sect{\ref{sec:education-eval-bugfinding}},
I selected 3 bugs to study in depth to ascertain
whether Landslide played an instrumental difference
in helping the students ultimately submit respecitvely correct implementations.
To avoid bias of picking too obscure and/or trivial bugs
that Landslide alone might find but even course staff would not expect students to solve,
I chose only bugs which had substantial penalties in the grading rubric
(guided, as well, by my own intuition as a former TA).
% TODO: the results are shown in table...

{\bf Thread exit use-after-free.}
During {\tt thr\_exit()}, a thread must arrange for its own stack memory allocation to be freed.
A common student pitfall is to allow a {\tt thr\_join()}ing thread to free said stack space
while {\tt thr\_exit()} is still executing userspace C code (which inevitably accesses the stack),
or even for {\tt thr\_exit()} to do it itself.
In the former case, threads must interleave specifically to exhibit a use-after-free;
in the latter case, the use-after-free will be deterministic (i.e., present in all interleavings).
Besides Landslide, however,
the students have no Valgrind-like heap debugging tool which would report a bug immediately upon any illegal heap access;
without such, a third thread would have to race to recycle the memory for a new thread stack
and conflict with the old exiting thread
before any problem could be detected.
% TODO: mark which of the tej bugs were deterministic vs race
%
The Landslide-friendly test {\tt thr\_exit\_join} is most likely to expose this bug, and
the official rubric penalties for it are
{\tt :exit\_stack\_minor -2} and
{\tt :exit\_stack\_race -4} (for the former case),
and
{\tt :uses\_freed\_stack -4} (for the latter).

{\bf Paraguay.}
Named after a certain Paraguayan 15-410 TA from years gone by,
this refers to a condition variable bug in which a thread which sleeps on two different condition variables
can spuriously wake up early from the later {\tt cond\_wait()}.
The precise reasoning why many common na\"ive student implementations are susceptible to this bug,
as well as the 3 major ways of fixing it,
are one of 15-410 staff's closely-guarded secrets.
Suffice to say that the test invokes a specially-made Pathos {\tt misbehave} mode
which biases thread scheduling towards the specific interleaving required to exhibit the bug
(Landslide, of course, replaces this {\tt misbehav}iour with model checking).
% TODO: i feel tihs sentence should go somewhere else, like after analysis, not in description
Hence, despite being a subtle concurrency bug,
the official course test suite is likely to expose it,
so comparing how many Landslide users and non-users submitted this bug in particular
speaks more to the impact of Landslide's preemption traces
in helping students diagnose a bug that a ``stress'' test could already find.
%
The Landslide-friendly test {\tt paraguay} is most likely to expose this bug, and
the official rubric penalty for it is
{\tt :paraguay -2}.

{\bf R/W-lock downgrade.}
In addition to the standard R/W-lock interface,
P2 requires students to implement {\tt rwlock\_downgrade()}:
called with the lock held in write mode,
the caller adopts the reader role instead,
allowing other waiting reader threads to proceed simultaneously,
all while allowing no waiting writers to access in between.
The Landslide-friendly test {\tt rwlock\_downgrade\_read\_test}
checks that readers are allowed simultaneous access after a downgrade, and
the official rubric penalties related to this bug are
{\tt :downgrade\_no\_signal -1},
{\tt :unsafe\_rwlock\_downgrade -2},
{\tt :downgrade\_lies -3}, and
{\tt :downgrade\_deadlock -4}.

\subsection{Student opinions}

% TODO

\subsection{Other universities}

% TODO

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Discussion}

% TODO intro text

\subsection{Bias}

% TODO: addressing bias
% well, we did the best we could(?)
% survey email: "please answer honestly rather than flatteringly"
% anything else?

%%%% pintos %%%%
% survey questions i WISH i had asked
% - did you have any technical difficulties w landslide that i had to intervene on
% mb anything else from timmys 2nd latest email

\subsection{Retrospect}

Human subjects research is inherently messy.
More than just trying to draw firm conclusions from
the opinions of students who are just learning concurrency to begin with,
student feedback in turn guided the constant development
of Landslide, and the experimental design itself,
as the semesters went by.
In this section I will fantasize about how I might have run more perfect experiments
granted the impossible wish of knowing then what I know now.

\subsubsection{Pebbles}

% TODO

\subsubsection{Pintos}

I attribute the low participation rate of Pintos students to P2 students to two major factors:
one, not incentivizing the students to directly improve their grades
(instead offering only the vague promise of a ``learning experience'' debugging their code only after handin),
and two, not traveling to the university to introduce the research topic in an in-person lecture
(leaving the students potentially confused about what advantage, if any, was offered over stress testing).

While part of the point of this experimental design was to evaluate Landslide as a grading tool in the hands of TAs,
I would be remiss to mention that I also feared the automatic annotation process would not be as robust as the P2 version.
Indeed, while helping Kevin get oriented with using Landslide,
I implemented
several % TODO: how many and what specifically
fixes/improvements to the setup scripts
as we found student kernels that failed to automatically annotate
(for example, those with {\tt ready\_list} changed to an array,
as described in \sect{\ref{sec:education-pintos-instrumentation}}).
Had we given Landslide directly to students that semester,
the students themselves would have had to email me for tech support.

Hypothetically, I could have achieved greater user study participation
either by offering extra credit to students
or by offering an autograder-like interface for students to receive bug reports before their deadlines instead of after
(either way requiring a more rigorous IRB review process).
Practically, as for non-research use in Pintos classes,
Landslide can now handle a considerably wider variety of student implementation quirks
on account of this time's fixes.
In its current shape I would recommend it for TA use grading,
but not necessarily directly to students without someone familiar with the codebase on immediate hand for tech support.

% Point two is much more easily addressed, although perhaps not necessary if only being used for grading anyway.

